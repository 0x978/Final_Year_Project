{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 7293442,
     "sourceType": "datasetVersion",
     "datasetId": 4230174
    },
    {
     "sourceId": 7293534,
     "sourceType": "datasetVersion",
     "datasetId": 4230224
    },
    {
     "sourceId": 7300111,
     "sourceType": "datasetVersion",
     "datasetId": 4234911
    }
   ],
   "dockerImageVersionId": 30627,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  },
  "colab": {
   "provenance": [
    {
     "file_id": "1YlaJXSwDnBU706i8dKei6wRV00B9gJi_",
     "timestamp": 1703981202137
    }
   ]
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning led-large-book-summary on privacy policies.\n",
    "\n",
    "This notebook will go through data-processing and training of a the NLP model responsible for summarising privacy policies, using the \"led-large-book-summary\" as a base to be fine-tuned on my task of summarising privacy policies.\n",
    "\n",
    "led-large-book-summary: https://huggingface.co/pszemraj/led-large-book-summary\n",
    "\n",
    "**This model needs ~16GB of VRAM for training, running on any lower will give a CUDA out of memory error.**"
   ],
   "metadata": {
    "id": "xXkf41_oHI4e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-Processing\n",
    "First, import and install required libraries"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-28T17:51:33.438989Z",
     "iopub.execute_input": "2023-12-28T17:51:33.439335Z",
     "iopub.status.idle": "2023-12-28T17:51:33.445630Z",
     "shell.execute_reply.started": "2023-12-28T17:51:33.439306Z",
     "shell.execute_reply": "2023-12-28T17:51:33.444030Z"
    },
    "id": "oWzMAGcgHI4f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets\n",
    "!pip install -U accelerate\n",
    "!pip install -U transformers\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "# Allocate maximum CUDA memory reserve in an attempt to prevent CUDA out of memory errors\n",
    "# Reserve is simply the reserved memory, not the in-use memory.\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\""
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:24:59.482367900Z",
     "start_time": "2023-12-27T23:24:57.520190900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-12-30T00:45:39.462949Z",
     "iopub.execute_input": "2023-12-30T00:45:39.464431Z",
     "iopub.status.idle": "2023-12-30T00:46:40.616435Z",
     "shell.execute_reply.started": "2023-12-30T00:45:39.464380Z",
     "shell.execute_reply": "2023-12-30T00:46:40.614981Z"
    },
    "trusted": true,
    "id": "vaMwi-PfHI4f",
    "outputId": "be657b34-a761-4797-96f4-39635bbfbcae"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0+cpu)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.19.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\nCollecting transformers\n  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/20/0a/739426a81f7635b422fbe6cb8d1d99d1235579a6ac8024c13d743efa6847/transformers-4.36.2-py3-none-any.whl.metadata\n  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m126.8/126.8 kB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.2/8.2 MB\u001B[0m \u001B[31m55.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m:00:01\u001B[0m\n\u001B[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.36.0\n    Uninstalling transformers-4.36.0:\n      Successfully uninstalled transformers-4.36.0\nSuccessfully installed transformers-4.36.2\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, the dataset needs to be converted from JSON to a \"dataset\" object from the `datasets` library.\n",
    "\n",
    "This library provides a `train_test_split` to split the dataset into a test set and training set.\n",
    "\n",
    "I shuffle the dataset first with a fixed seed, so the results are always repoducible."
   ],
   "metadata": {
    "id": "X8e6PZD3HI4g"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Dataset files are stored in different locations depending on where the Notebook is ran\n",
    "# Uncomment depending on location:\n",
    "\n",
    "# Kaggle:\n",
    "dataset_location = \"/kaggle/input/nlp-attempt-2/dataset.json\"\n",
    "\n",
    "# Google Colab / Running Locally:\n",
    "# dataset_location = \"dataset.json\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=dataset_location,split='train')\n",
    "dataset = dataset.shuffle(seed=2424)\n",
    "dataset = dataset.train_test_split(test_size=0.1, shuffle=False) # disabling shuffling to shuffle with a fixed seed on previous line instead\n",
    "print(dataset)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:00.536602700Z",
     "start_time": "2023-12-27T23:24:59.485367Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-12-30T00:46:40.619330Z",
     "iopub.execute_input": "2023-12-30T00:46:40.620993Z",
     "iopub.status.idle": "2023-12-30T00:46:41.655767Z",
     "shell.execute_reply.started": "2023-12-30T00:46:40.620926Z",
     "shell.execute_reply": "2023-12-30T00:46:41.654393Z"
    },
    "trusted": true,
    "colab": {
     "referenced_widgets": [
      "8242505fb73243f6826276293e8d54b3",
      "ab0354e5cf67432e916af6c9b770fa4f"
     ]
    },
    "id": "tSJqUhC3HI4g",
    "outputId": "72c0ab8f-4e8e-4b9b-8c8d-b77aeced1c75"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-5e72351582b235dd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8242505fb73243f6826276293e8d54b3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab0354e5cf67432e916af6c9b770fa4f"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5e72351582b235dd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\nDatasetDict({\n    train: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 217\n    })\n    test: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 25\n    })\n})\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset has been split into two sets:\n",
    " - Train\n",
    " - Test\n",
    "\n",
    "The train set will be used to train the model - this is the information the model will learn from.\n",
    "The test set will be used to test the model after training, to see how it performs for some data **It has never seen**\n",
    "\n",
    "The dataset has two features - the \"document\" which is a privacy policy and then \"summary\" is the respective summary"
   ],
   "metadata": {
    "id": "ABu90dnNHI4h"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, a model needs to be selected to conduct transfer learning on.\n",
    "\n",
    "There is a problem here, in that, the collected privacy policies and terms and conditions are **very long**.\n",
    "\n",
    "Below, the length of the first item in the dataset is 167,601 characters whilst the largest is 644,722 characters."
   ],
   "metadata": {
    "id": "hA05kZljHI4h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "length_of_first_item = len(dataset['train'][0]['document'])\n",
    "print(f'The length of the first privacy policy in the train dataset is {length_of_first_item} characters')\n",
    "\n",
    "length_of_longest_document = len(max(dataset['train'], key=lambda x: len(x['document']))['document'])\n",
    "print(f'Length of the longest privacy policy in the train dataset is {length_of_longest_document} characters')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:00.583600500Z",
     "start_time": "2023-12-27T23:25:00.539603100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-12-30T00:46:41.657888Z",
     "iopub.execute_input": "2023-12-30T00:46:41.658713Z",
     "iopub.status.idle": "2023-12-30T00:46:41.739905Z",
     "shell.execute_reply.started": "2023-12-30T00:46:41.658659Z",
     "shell.execute_reply": "2023-12-30T00:46:41.738887Z"
    },
    "trusted": true,
    "id": "CVM3LOW9HI4h",
    "outputId": "eb35afb5-9bda-455a-9ea6-bae4ea23e5de"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "The length of the first privacy policy in the train dataset is 167601 characters\nLength of the longest privacy policy in the train dataset is 644722 characters\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is problematic as NLP models have a **maximum token count** that it can handle, often much lower than the length of the collected documents, exacerbated by the fact summarisation models typically have lower maximum token counts.\n",
    "\n",
    "This may end up affecting the accuracy of the model and it's ability to learn from the data, as if a portion of the document is cut off to stay within the maximum token count, the summary may not fully match the document.\n",
    "\n",
    "Unfortunately, this is a limitation of NLP as a whole.\n",
    "\n",
    "The maximum length of most of the most common summarization models is 1024 tokens, it's impossible to tell how many characters this is, but a rough heuristic is [1 token = 4 characters](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)\n",
    "\n",
    "The model below thus supports around 4096 characters, which is clearly not good enough for the data."
   ],
   "metadata": {
    "id": "Fi5EjEy8HI4h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Commented out to save memory for training...\n",
    "\n",
    "# bart_model_checkpoint_name = \"facebook/bart-large-cnn\"\n",
    "# bart_tokenizer = AutoTokenizer.from_pretrained(bart_model_checkpoint_name)\n",
    "# bart_max_length = bart_tokenizer.model_max_length\n",
    "# bart_max_length, bart_max_length*4"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:02.663153500Z",
     "start_time": "2023-12-27T23:25:00.587603200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-12-30T00:46:41.742565Z",
     "iopub.execute_input": "2023-12-30T00:46:41.743865Z",
     "iopub.status.idle": "2023-12-30T00:46:47.548360Z",
     "shell.execute_reply.started": "2023-12-30T00:46:41.743819Z",
     "shell.execute_reply": "2023-12-30T00:46:47.546977Z"
    },
    "trusted": true,
    "id": "leHkh2jrHI4h"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thus, it's required to look at a bigger model.\n",
    "\n",
    "Below, \"led-large-book-summary\" has a model max length of 16384 tokens, applying the same heuristic as before, this is about 65536 words.\n",
    "\n",
    "This model was trained on the [BookSum dataset](https://arxiv.org/abs/2105.08209) which contains \"plays, short stories, and novels\" with expired copyright, and the aim of the training was to produce valuable summaries of the given documents.\n",
    "\n",
    "I will conduct transfer learning to remove the head of this model which is specifically focused on books, and train on the new task of summarising **privacy policies**"
   ],
   "metadata": {
    "id": "tcLWOMaNHI4h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_checkpoint_name = \"pszemraj/led-large-book-summary\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_checkpoint_name)\n",
    "tokeniser.model_max_length, tokeniser.model_max_length * 4"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:02.850129500Z",
     "start_time": "2023-12-27T23:25:02.666127200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-12-30T00:46:47.549880Z",
     "iopub.execute_input": "2023-12-30T00:46:47.550501Z",
     "iopub.status.idle": "2023-12-30T00:46:49.610080Z",
     "shell.execute_reply.started": "2023-12-30T00:46:47.550464Z",
     "shell.execute_reply": "2023-12-30T00:46:49.608135Z"
    },
    "trusted": true,
    "colab": {
     "referenced_widgets": [
      "15524d688ac64022a2d507ddbd98aa30",
      "92f9fa1b91554d258a186a8809fd43d5",
      "32898a6b4279475fa45980b890c2534d",
      "70f5b0a9a16a4d909b3b074b156b07fd",
      "07fe52c512e040ab9241e27308d842c3"
     ]
    },
    "id": "Z6BmCRTnHI4i",
    "outputId": "c115ba98-179d-49c3-c01d-33dbe66c95d8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15524d688ac64022a2d507ddbd98aa30"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92f9fa1b91554d258a186a8809fd43d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32898a6b4279475fa45980b890c2534d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70f5b0a9a16a4d909b3b074b156b07fd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07fe52c512e040ab9241e27308d842c3"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "(16384, 65536)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above has taken the \"tokeniser\" from the model.\n",
    "\n",
    "Transformer models only take numerical inputs, thus, it's essential to get a numerical representation of the input.\n",
    "\n",
    "The tokeniser is responsible for turning sentences into a series of numbers, called \"tokens\".\n",
    "\n",
    "Tokenisers also use special characters, often to indicate the start and end of sentences or words.\n",
    "\n",
    "All this information helps a NLP neural network form an understanding of the input.\n",
    "\n",
    "The below output shows the tokens used to encode the given test string, with `<s>` being used to indicate the start of a sequence, and each subsequent token beginning with `Ġ`, indicating the start of a new token.\n",
    "\n",
    "Note that `tokeniser` was split into two tokens `token` and `iser`, this is important for tokenisers to be able to re-use tokens wherever possible - `iser` can be used as the suffix for many different words.\n",
    "\n",
    "An attention mask can tell the tokenizer not to pay attention to certain tokens."
   ],
   "metadata": {
    "id": "-7eEh3rrHI4i"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_string = tokeniser(\"This is a test string to test out the tokeniser\")\n",
    "print(test_string)\n",
    "tokeniser.convert_ids_to_tokens(test_string.input_ids)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:02.865783Z",
     "start_time": "2023-12-27T23:25:02.853128700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-12-30T00:46:49.611966Z",
     "iopub.execute_input": "2023-12-30T00:46:49.612456Z",
     "iopub.status.idle": "2023-12-30T00:46:49.631418Z",
     "shell.execute_reply.started": "2023-12-30T00:46:49.612418Z",
     "shell.execute_reply": "2023-12-30T00:46:49.630213Z"
    },
    "trusted": true,
    "id": "hKkcJeGPHI4i",
    "outputId": "109399b4-e905-4d18-a381-8d36b2042ec5"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'input_ids': [0, 713, 16, 10, 1296, 6755, 7, 1296, 66, 5, 19233, 5999, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
     "output_type": "stream"
    },
    {
     "execution_count": 6,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['<s>',\n 'This',\n 'Ġis',\n 'Ġa',\n 'Ġtest',\n 'Ġstring',\n 'Ġto',\n 'Ġtest',\n 'Ġout',\n 'Ġthe',\n 'Ġtoken',\n 'iser',\n '</s>']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, recall that the maximum token size for the model was 16384, or about 65536 words.\n",
    "\n",
    "The majority of the scraped documents still exceed this amount, unfortunately, since no bigger summarisation model exists, the input text will need to be truncated down to the maximum token size, which **will have an impact on the model's ability to learn** but there is no other option.\n",
    "\n",
    "Furthermore, the tokenizer will output may different outputs, one of which being `input_ids`, the numerical IDs of the tokens in the tokenised text.\n",
    "\n",
    "The `input_ids` will need to be assigned to the `labels` key of the tokenised input document. This prepares the tokenised input for a task where the model needs to predict the summary, which is how the model will learn.\n",
    "\n",
    "Below defines and runs a function which will truncate the input document, **but not the output** and assign the labels and input id's as necessary"
   ],
   "metadata": {
    "id": "DR0hYdFbHI4i"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def truncate_input_tokens(input):\n",
    "    truncated_input = tokeniser(\n",
    "        input[\"document\"],\n",
    "        max_length = 16384,\n",
    "        truncation = True\n",
    "    )\n",
    "    labels = tokeniser( # don't truncate output\n",
    "        input[\"summary\"],\n",
    "        truncation = False,\n",
    "    )\n",
    "    truncated_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return truncated_input\n",
    "\n",
    "# By passing `batched = true` into the map function, more than one item is applied to the function at a time.\n",
    "tokenised_dataset = dataset.map(truncate_input_tokens, batched = True)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:07.763873200Z",
     "start_time": "2023-12-27T23:25:02.868783400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-12-30T00:46:49.633944Z",
     "iopub.execute_input": "2023-12-30T00:46:49.635758Z",
     "iopub.status.idle": "2023-12-30T00:47:02.821462Z",
     "shell.execute_reply.started": "2023-12-30T00:46:49.635697Z",
     "shell.execute_reply": "2023-12-30T00:47:02.819922Z"
    },
    "trusted": true,
    "colab": {
     "referenced_widgets": [
      "e6080961f1b346e89d7b0b42ba94a342",
      "be66a4016b3143359b7db29b1a2f8ad9"
     ]
    },
    "id": "m0uAAFQJHI4i",
    "outputId": "29e954d8-fd67-4dec-c4c5-57f1d993ee72"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6080961f1b346e89d7b0b42ba94a342"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be66a4016b3143359b7db29b1a2f8ad9"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below now looks at the new dataset.\n",
    "\n",
    "We can see new features `input_ids`, `attention_mask` and `labels`\n",
    "\n",
    "The lengths of these inputs are also visible, both at 16384 (the maximum) tokens for the first input, and the biggest.\n",
    "\n",
    "The smallest input however, is just 393 tokens, suggesting some documents stayed well under the maximum."
   ],
   "metadata": {
    "id": "gL0I04KMHI4i"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(tokenised_dataset)\n",
    "\n",
    "length_of_first_item = len(tokenised_dataset['train'][0]['input_ids'])\n",
    "print(f'The length of the first privacy policy in the train dataset is {length_of_first_item} tokens')\n",
    "\n",
    "length_of_longest_document = len(max(tokenised_dataset['train'], key=lambda x: len(x['input_ids']))['input_ids'])\n",
    "print(f'Length of the longest privacy policy in the train dataset is {length_of_longest_document} tokens')\n",
    "\n",
    "length_of_longest_document = len(min(tokenised_dataset['train'], key=lambda x: len(x['input_ids']))['input_ids'])\n",
    "print(f'Length of the smallest privacy policy in the train dataset is {length_of_longest_document} tokens')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:12.004420400Z",
     "start_time": "2023-12-27T23:25:07.760854Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-12-30T00:47:02.823596Z",
     "iopub.execute_input": "2023-12-30T00:47:02.824023Z",
     "iopub.status.idle": "2023-12-30T00:47:11.154487Z",
     "shell.execute_reply.started": "2023-12-30T00:47:02.823987Z",
     "shell.execute_reply": "2023-12-30T00:47:11.153326Z"
    },
    "trusted": true,
    "id": "Ha-K3KjLHI4j",
    "outputId": "89866b24-e079-41e6-9a19-faf282bdffaf"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "DatasetDict({\n    train: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 217\n    })\n    test: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 25\n    })\n})\nThe length of the first privacy policy in the train dataset is 16384 tokens\nLength of the longest privacy policy in the train dataset is 16384 tokens\nLength of the smallest privacy policy in the train dataset is 393 tokens\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining Evaluation Techniques"
   ],
   "metadata": {
    "id": "a-7itTeDHI4j"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In Evaluation of summarisation tasks in NLP, the ROUGE metric is most often used.\n",
    "\n",
    "To evaluate the model, I will use the `Hugging Face evaluate` library, as well as the `rouge-score` library to produce the rouge score for the model's summaries."
   ],
   "metadata": {
    "id": "hN6qvjy5HI4j"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:20.164335900Z",
     "start_time": "2023-12-27T23:25:12.005422Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-12-30T00:47:11.156022Z",
     "iopub.execute_input": "2023-12-30T00:47:11.157544Z",
     "iopub.status.idle": "2023-12-30T00:48:02.705477Z",
     "shell.execute_reply.started": "2023-12-30T00:47:11.157497Z",
     "shell.execute_reply": "2023-12-30T00:48:02.703841Z"
    },
    "trusted": true,
    "colab": {
     "referenced_widgets": [
      "c90cb8498e774aeb8573a3902556b691"
     ]
    },
    "id": "VpR0Ot-THI4j",
    "outputId": "4dd12ea0-69a0-4865-f9c0-ccb334917f5f"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Collecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.12.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.24.3)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=e1fcdb127b1b3879daf187744870b8f760a2b3a77d64f78b70f894ea1c26b8d4\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c90cb8498e774aeb8573a3902556b691"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To calculate the ROUGE metric on predictions, we need the **decoded** predictions and the **decoded** labels, as they are currently *tokenised* (labels simply refers to the expected values).\n",
    "\n",
    "Thus, we need to decode the input tokens back into their English representations.\n",
    "\n",
    "The `batch-decode` function does this, with the parameter `skip_special_tokens` specifying to not handle any special tokens such as `<s>` indicating the start of a sequence.\n",
    "\n",
    "The function below will decode and find the ROUGE score."
   ],
   "metadata": {
    "id": "KWdP6bYnHI4j"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def decode_and_find_rouge(result):\n",
    "    predictions,labels = result # extract predictions and labels from the passed in result.\n",
    "\n",
    "    # Decode by converting tokenised inputs back into English representations\n",
    "    decoded_predictions = tokeniser.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokeniser.batch_decode(labels, skip_special_tokens = True)\n",
    "\n",
    "    rouge_score = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "\n",
    "    # Find average length of generated text by removing non-zero and padding ids, and take the mean.\n",
    "    num_of_predictions = [np.count_nonzero(pred != tokeniser.pad_token_id) for pred in predictions]\n",
    "    rouge_score[\"gen_len\"] = np.mean(num_of_predictions) # The average is stored in dict with key \"gen_len\"\n",
    "\n",
    "    return {k: round(v, 4) for k, v in rouge_score.items()} # round all rouge score outputs to 4 decimal places."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:20.179299900Z",
     "start_time": "2023-12-27T23:25:20.167300500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-12-30T00:48:02.710015Z",
     "iopub.execute_input": "2023-12-30T00:48:02.711189Z",
     "iopub.status.idle": "2023-12-30T00:48:02.720966Z",
     "shell.execute_reply.started": "2023-12-30T00:48:02.711139Z",
     "shell.execute_reply": "2023-12-30T00:48:02.719204Z"
    },
    "trusted": true,
    "id": "MWkDQeGWHI4j"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the Model"
   ],
   "metadata": {
    "id": "LrwGkA5QHI4j"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, the model hyper-paramters need to be defined.\n",
    "\n",
    "This is a very expensive model on memory and it is essential to avoid exceeding the GPU's VRAM, thus the hyper-parameters are configured as such:\n",
    "\n",
    "1) The batch size will be set to just 1 to reduce the amount of data stored in memory.\n",
    "\n",
    "2) 16-bit floating point precision will be used, rather than 32-bit.\n",
    "\n",
    "3) The optimiser \"adafactor\" is used over the industry standard of \"Adam\". As a result, the model converges slower but also uses less memory.\n",
    "\n",
    "4) \"Gradient Checkpointing\" is used, instructing the model to forget the majority of forward-pass activations and instead recompute them on demand during the backward pass, saving only the \"most important\" activations. Significantly slower training time, but also saves a lot of VRAM.\n",
    "\n",
    "5) \"Gradient Accumulation\" is used simulate a \"larger\" effective batch size. Instead of updating the model's parameters after processing eachbatch, the gradients are accumulated over 2 batches before performing a single update."
   ],
   "metadata": {
    "id": "41LZtgAUHI4j"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint_name)\n",
    "\n",
    "# Creating a data collator, which will form the batches which will be fed to the model.\n",
    "# It will also conduct padding if necessary to get all inputs of equal length.\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokeniser, model=model_checkpoint_name)\n",
    "\n",
    "models_arguments = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"trained_model\",\n",
    "    evaluation_strategy=\"epoch\", # Run evaluation function on each epoch\n",
    "    learning_rate=2e-5, # learning rate hyperparameter set to 0.00002\n",
    "    per_device_train_batch_size= 1, # split into batches of 1 for training\n",
    "    per_device_eval_batch_size=1, # split to batches of 1 for evaluation\n",
    "    weight_decay=0.01, # Utilises L2 regularisation in an attempt to prevent overfitting\n",
    "    save_total_limit=3, # save 3 checkpoints only and delete older checkpoints (Kept using all RAM without)\n",
    "    num_train_epochs=4, # train for 4 epochs\n",
    "    predict_with_generate=True, # Generate summaries for each input ; essential for summarisation tasks\n",
    "    fp16=True, # use 16 bit floating point - reduced memory usage\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\"\n",
    ")\n",
    "\n",
    "# Collect the previously defined trainer parameters, such as the evaluation technique, tokeniser and datasets.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=models_arguments,\n",
    "    train_dataset=tokenised_dataset[\"train\"],\n",
    "    eval_dataset=tokenised_dataset[\"test\"],\n",
    "    tokenizer=tokeniser,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=decode_and_find_rouge,\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:24.378548100Z",
     "start_time": "2023-12-27T23:25:20.183300400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-12-28T22:32:58.457889Z",
     "iopub.execute_input": "2023-12-28T22:32:58.458281Z",
     "iopub.status.idle": "2023-12-28T22:33:40.804278Z",
     "shell.execute_reply.started": "2023-12-28T22:32:58.458247Z",
     "shell.execute_reply": "2023-12-28T22:33:40.803395Z"
    },
    "trusted": true,
    "colab": {
     "referenced_widgets": [
      "714301834ebe4c2381a820aaaf8ce167",
      "58c3c0c7b82f4983a0d9d5cf8704d58c"
     ]
    },
    "id": "iQOcMjqtHI4k",
    "outputId": "f644efd4-02be-4071-dce7-7bd7ebcb6a86"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "714301834ebe4c2381a820aaaf8ce167"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58c3c0c7b82f4983a0d9d5cf8704d58c"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, the model can be ran using the `train()` function on the trainer.\n",
    "\n",
    "After training, the model will be saved in a file called \"trained_model\".\n",
    " - The size of the model is ~1.7GB"
   ],
   "metadata": {
    "id": "FWzFvgOzHI4k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Uncomment below line if using Kaggle.\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"trained_model\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:26:16.985454Z",
     "start_time": "2023-12-27T23:25:42.912321500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-12-28T22:33:40.805480Z",
     "iopub.execute_input": "2023-12-28T22:33:40.805780Z",
     "iopub.status.idle": "2023-12-29T00:47:58.947703Z",
     "shell.execute_reply.started": "2023-12-28T22:33:40.805754Z",
     "shell.execute_reply": "2023-12-29T00:47:58.946531Z"
    },
    "trusted": true,
    "id": "JGXsMyW_HI4k",
    "outputId": "5519be30-08e5-48a8-e8ff-60afe182f895"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a LEDTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [432/432 2:13:23, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>0.708182</td>\n      <td>0.433700</td>\n      <td>0.271100</td>\n      <td>0.223100</td>\n      <td>0.223500</td>\n      <td>526.920000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.561906</td>\n      <td>0.516600</td>\n      <td>0.312100</td>\n      <td>0.259300</td>\n      <td>0.259600</td>\n      <td>166.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.551391</td>\n      <td>0.519400</td>\n      <td>0.325000</td>\n      <td>0.266100</td>\n      <td>0.266400</td>\n      <td>160.920000</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating the model\n",
    "\n",
    "First, we can observe the output of the model on some examples, to see how the model is handling privacy policies."
   ],
   "metadata": {
    "id": "QwnLuPcOHI4k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel,LEDTokenizer, LEDForConditionalGeneration\n",
    "\n",
    "trained_tokeniser = LEDTokenizer.from_pretrained(\"/kaggle/input/model-trained/Model stuff\")\n",
    "\n",
    "trained_model = LEDForConditionalGeneration.from_pretrained(\"/kaggle/input/model-trained/Model stuff\")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=trained_model,tokenizer=trained_tokeniser)\n",
    "\n",
    "\n",
    "example = dataset[\"test\"][\"document\"][0]\n",
    "\n",
    "tokenised_example = trained_tokeniser(example,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "\n",
    "outputs = trained_model.generate(tokenised_example)\n",
    "trained_tokeniser.decode(outputs[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T00:48:17.760317Z",
     "iopub.execute_input": "2023-12-30T00:48:17.760775Z",
     "iopub.status.idle": "2023-12-30T00:54:25.485044Z",
     "shell.execute_reply.started": "2023-12-30T00:48:17.760732Z",
     "shell.execute_reply": "2023-12-30T00:54:25.483181Z"
    },
    "trusted": true,
    "id": "p7JgfUksHI4k",
    "outputId": "0fa062de-5d5d-4573-9bd8-7f91390b86c5"
   },
   "execution_count": null,
   "outputs": [
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'The service provides details about what kinds of personal information they collect. This service is only available to users over a certain age. You can opt out of promotional communications. The service may use tracking pixels, web beacons, browser fingerprinting, and/or device fingerprinting on users.. Third-party cookies are used for statistics. Do Not Track (DNT) headers are ignored and you are tracked anyway even if you set this header.. There is a date of the last update of the agreements. Information is provided about how your personal data is used. Your IP address is collected, which can be used to view your approximate location. Many different types of personal data are collected'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "second_example = dataset[\"test\"][\"document\"][1]\n",
    "\n",
    "tokenised_example_2 = trained_tokeniser(second_example,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "outputs_2 = trained_model.generate(tokenised_example_2)\n",
    "trained_tokeniser.decode(outputs_2[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-29T22:12:16.009571Z",
     "iopub.execute_input": "2023-12-29T22:12:16.010280Z",
     "iopub.status.idle": "2023-12-29T22:16:47.659597Z",
     "shell.execute_reply.started": "2023-12-29T22:12:16.010246Z",
     "shell.execute_reply": "2023-12-29T22:16:47.658581Z"
    },
    "trusted": true,
    "id": "sAexkreeHI4k",
    "outputId": "166c602e-d687-4a94-b1ef-8ccc807900bf"
   },
   "execution_count": null,
   "outputs": [
    {
     "execution_count": 13,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'The service provides details about what kinds of personal information they collect. You can request access and deletion of personal data. The service may use tracking pixels, web beacons, browser fingerprinting, and/or device fingerprinting on users.. This service reserves the right to disclose your personal information without notifying you. Blocking first party cookies may limit your ability to use the service. Tracking via third-party cookies for other purposes without your consent.. Your IP address is collected, which can be used to view your approximate location. Information is provided about how your personal data is used.  Terms may be changed any time at their discretion, without notice to you. There is a date of the last update of the agreements. Do Not Track (DNT) headers are ignored and you are tracked anyway even if you set this header.. Logs are kept for an undefined period of time. Third parties used by the service are bound by confidentiality obligations'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "lorem_example = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\"\n",
    "\n",
    "tokenised_example_lorem = trained_tokeniser(lorem_example,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "outputs_lorem = trained_model.generate(tokenised_example_lorem)\n",
    "trained_tokeniser.decode(outputs_lorem[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-29T04:00:40.919256Z",
     "iopub.execute_input": "2023-12-29T04:00:40.919653Z",
     "iopub.status.idle": "2023-12-29T04:00:59.450874Z",
     "shell.execute_reply.started": "2023-12-29T04:00:40.919621Z",
     "shell.execute_reply": "2023-12-29T04:00:59.449735Z"
    },
    "trusted": true,
    "id": "J1BUe1aiHI4l",
    "outputId": "72217ff6-f8b8-450e-d0bf-6f0e51a85800"
   },
   "execution_count": null,
   "outputs": [
    {
     "execution_count": 31,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmodium tempor incididunt ut labore et dolore magna aliqua. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolor eu fugiat nulla pariatur. Except for cases of \"exceptional sint occaecat cupidatat non proident\"  which are rare, this service does not collect, use, or share location data.'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the surface these summaries look sufficient.\n",
    "\n",
    "We can look at the ROUGE scores of a test set example:"
   ],
   "metadata": {
    "id": "q6UQNhdNHI4l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "example_document = dataset[\"test\"][\"document\"][5]\n",
    "ground_truth_summary = dataset[\"test\"][\"summary\"][5]\n",
    "\n",
    "tokenised_document = trained_tokeniser(example_document,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "tokenised_ground_truth = trained_tokeniser(ground_truth_summary,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "output_summary = trained_model.generate(tokenised_document)\n",
    "\n",
    "decoded_summary = trained_tokeniser.decode(output_summary[0], skip_special_tokens=True)\n",
    "\n",
    "# Rouge metric expects array.\n",
    "truth = [ground_truth_summary]\n",
    "summary = [decoded_summary]\n",
    "\n",
    "rouge_score = rouge.compute(predictions=truth, references=summary)\n",
    "\n",
    "print(rouge_score)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-29T22:50:50.287602Z",
     "iopub.execute_input": "2023-12-29T22:50:50.288347Z",
     "iopub.status.idle": "2023-12-29T22:54:03.285089Z",
     "shell.execute_reply.started": "2023-12-29T22:50:50.288314Z",
     "shell.execute_reply": "2023-12-29T22:54:03.284010Z"
    },
    "trusted": true,
    "id": "K6BeR5gLHI4l",
    "outputId": "e56c47bf-17d4-4c9e-ea41-49783cd68573"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'rouge1': 0.4848484848484849, 'rouge2': 0.2959183673469388, 'rougeL': 0.2828282828282828, 'rougeLsum': 0.2828282828282828}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now take the ROUGE score across the entire test set\n",
    "\n",
    "This will take a while, due to needing to create summaries for the entire test set."
   ],
   "metadata": {
    "id": "TrPjWnVwHI4l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "summaries = []\n",
    "truths = []\n",
    "count = 1\n",
    "\n",
    "for item in dataset[\"test\"]:\n",
    "    print(f'Processing document {count} of {len(dataset[\"test\"])}')\n",
    "    document = item[\"document\"]\n",
    "    ground_truth_summary = item[\"summary\"]\n",
    "\n",
    "    tokenised_document = trained_tokeniser(document,return_tensors=\"pt\",truncation=True).input_ids\n",
    "    generated_summary = trained_model.generate(tokenised_document)\n",
    "    decoded_summary = trained_tokeniser.decode(generated_summary[0], skip_special_tokens=True)\n",
    "\n",
    "    summaries.append(decoded_summary)\n",
    "    truths.append(ground_truth_summary)\n",
    "    count += 1\n",
    "\n",
    "\n",
    "print(f'{len(summaries)} summaries produced and {len(truths)} ground truths collected')\n",
    "\n",
    "rouge_score = rouge.compute(predictions=summaries,references=truths)\n",
    "print(rouge_score)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T00:56:47.445050Z",
     "iopub.execute_input": "2023-12-30T00:56:47.445553Z",
     "iopub.status.idle": "2023-12-30T02:46:19.584528Z",
     "shell.execute_reply.started": "2023-12-30T00:56:47.445509Z",
     "shell.execute_reply": "2023-12-30T02:46:19.583150Z"
    },
    "trusted": true,
    "id": "eV8v6u6qHI4l",
    "outputId": "abdef849-4115-4fdb-a3b9-d0039f0fcdff"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Processing document 1 of 25\nProcessing document 2 of 25\nProcessing document 3 of 25\nProcessing document 4 of 25\nProcessing document 5 of 25\nProcessing document 6 of 25\nProcessing document 7 of 25\nProcessing document 8 of 25\nProcessing document 9 of 25\nProcessing document 10 of 25\nProcessing document 11 of 25\nProcessing document 12 of 25\nProcessing document 13 of 25\nProcessing document 14 of 25\nProcessing document 15 of 25\nProcessing document 16 of 25\nProcessing document 17 of 25\nProcessing document 18 of 25\nProcessing document 19 of 25\nProcessing document 20 of 25\nProcessing document 21 of 25\nProcessing document 22 of 25\nProcessing document 23 of 25\nProcessing document 24 of 25\nProcessing document 25 of 25\n25 summaries produced and 25 ground truths collected\n{'rouge1': 0.5286043133281195, 'rouge2': 0.3339869674376571, 'rougeL': 0.27132642421285524, 'rougeLsum': 0.2707264531675879}\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
