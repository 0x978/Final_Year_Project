{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7293442,
     "sourceType": "datasetVersion",
     "datasetId": 4230174
    },
    {
     "sourceId": 7293534,
     "sourceType": "datasetVersion",
     "datasetId": 4230224
    },
    {
     "sourceId": 7300111,
     "sourceType": "datasetVersion",
     "datasetId": 4234911
    },
    {
     "sourceId": 7320296,
     "sourceType": "datasetVersion",
     "datasetId": 4248138
    }
   ],
   "dockerImageVersionId": 30626,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning led-large-book-summary on fine-tuning terms and conditions.\n",
    "\n",
    "This notebook will go through data-processing and training of a the NLP model responsible for summarising terms and conditions, using the \"led-large-book-summary\" as a base model to be fine-tuned.\n",
    "\n",
    "led-large-book-summary: https://huggingface.co/pszemraj/led-large-book-summary\n",
    "\n",
    "This will be a very similar procedure to training on privacy policies, but with a dataset containing terms and conditons and their respective summaries.\n",
    "\n",
    "**This model needs ~16GB of VRAM for training, running on any lower will give a CUDA out of memory error.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-Processing\n",
    "First, import and install required libraries"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-28T17:51:33.438989Z",
     "iopub.execute_input": "2023-12-28T17:51:33.439335Z",
     "iopub.status.idle": "2023-12-28T17:51:33.445630Z",
     "shell.execute_reply.started": "2023-12-28T17:51:33.439306Z",
     "shell.execute_reply": "2023-12-28T17:51:33.444030Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets\n",
    "!pip install -U accelerate\n",
    "!pip install -U transformers\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "# Allocate maximum CUDA memory reserve in an attempt to prevent CUDA out of memory errors\n",
    "# Reserve is simply the reserved memory, not the in-use memory.\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:24:59.482367900Z",
     "start_time": "2023-12-27T23:24:57.520190900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-02T00:48:51.702204Z",
     "iopub.execute_input": "2024-01-02T00:48:51.703101Z",
     "iopub.status.idle": "2024-01-02T00:49:27.677771Z",
     "shell.execute_reply.started": "2024-01-02T00:48:51.703065Z",
     "shell.execute_reply": "2024-01-02T00:49:27.676754Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.19.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, the dataset needs to be converted from JSONL to a \"dataset\" object from the `datasets` library.\n",
    "\n",
    "This library provides a `train_test_split` to split the dataset into a test set and training set.\n",
    "\n",
    "I shuffle the dataset first with a fixed seed, so the results are always repoducible."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Dataset files are stored in different locations depending on where the Notebook is ran\n",
    "# Uncomment depending on location:\n",
    "\n",
    "# Kaggle:\n",
    "dataset_location = \"/kaggle/input/terms-jsonl/Terms_dataset.jsonl\"\n",
    "\n",
    "# Google Colab / Running Locally:\n",
    "# dataset_location = \"dataset.json\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=dataset_location,split='train')\n",
    "dataset = dataset.shuffle(seed=2424)\n",
    "dataset = dataset.train_test_split(test_size=0.1, shuffle=False) # disabling shuffling to shuffle with a fixed seed on previous line instead\n",
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:00.536602700Z",
     "start_time": "2023-12-27T23:24:59.485367Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-02T00:49:27.680509Z",
     "iopub.execute_input": "2024-01-02T00:49:27.680998Z",
     "iopub.status.idle": "2024-01-02T00:49:27.831657Z",
     "shell.execute_reply.started": "2024-01-02T00:49:27.680961Z",
     "shell.execute_reply": "2024-01-02T00:49:27.830810Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": "DatasetDict({\n    train: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 258\n    })\n    test: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 29\n    })\n})\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset has been split into two sets:\n",
    " - Train\n",
    " - Test\n",
    "\n",
    "The train set will be used to train the model - this is the information the model will learn from.\n",
    "The test set will be used to test the model after training, to see how it performs for some data **It has never seen**\n",
    "\n",
    "The dataset has two features - the \"document\" which is a terms document and then \"summary\" is the respective summary of thwese terms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, a model needs to be selected to conduct transfer learning on.\n",
    "\n",
    "There is a problem here, in that, the collected terms and conditions are **very long**.\n",
    "\n",
    "Below, the length of the first item in the dataset is 81,271 characters whilst the largest is 771,229 characters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "length_of_first_item = len(dataset['train'][0]['document'])\n",
    "print(f'The length of the first terms and conditions in the train dataset is {length_of_first_item} characters')\n",
    "\n",
    "length_of_longest_document = len(max(dataset['train'], key=lambda x: len(x['document']))['document'])\n",
    "print(f'Length of the longest terms and conditions in the train dataset is {length_of_longest_document} characters')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:00.583600500Z",
     "start_time": "2023-12-27T23:25:00.539603100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-02T00:49:27.832912Z",
     "iopub.execute_input": "2024-01-02T00:49:27.833184Z",
     "iopub.status.idle": "2024-01-02T00:49:27.909923Z",
     "shell.execute_reply.started": "2024-01-02T00:49:27.833158Z",
     "shell.execute_reply": "2024-01-02T00:49:27.909068Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": "The length of the first terms and conditions in the train dataset is 81271 characters\nLength of the longest terms and conditions in the train dataset is 771229 characters\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is problematic as NLP models have a **maximum token count** that it can handle, often much lower than the length of the collected documents, exacerbated by the fact summarisation models typically have lower maximum token counts.\n",
    "\n",
    "This may end up affecting the accuracy of the model and it's ability to learn from the data, as if a portion of the document is cut off to stay within the maximum token count, the summary may not fully match the document.\n",
    "\n",
    "Unfortunately, this is a limitation of NLP as a whole.\n",
    "\n",
    "The maximum length of most of the most common summarization models is 1024 tokens, it's impossible to tell how many characters this is, but a rough heuristic is [1 token = 4 characters](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) \n",
    "\n",
    "The model below thus supports around 4096 characters, which is clearly not good enough for the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Commented out to save memory for training...\n",
    "\n",
    "# bart_model_checkpoint_name = \"facebook/bart-large-cnn\"\n",
    "# bart_tokenizer = AutoTokenizer.from_pretrained(bart_model_checkpoint_name)\n",
    "# bart_max_length = bart_tokenizer.model_max_length\n",
    "# bart_max_length, bart_max_length*4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:02.663153500Z",
     "start_time": "2023-12-27T23:25:00.587603200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-02T00:49:27.911957Z",
     "iopub.execute_input": "2024-01-02T00:49:27.912243Z",
     "iopub.status.idle": "2024-01-02T00:49:27.916419Z",
     "shell.execute_reply.started": "2024-01-02T00:49:27.912218Z",
     "shell.execute_reply": "2024-01-02T00:49:27.915553Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thus, it's required to look at a bigger model.\n",
    "\n",
    "Below, \"led-large-book-summary\" has a model max length of 16384 tokens, applying the same heuristic as before, this is about 65536 words.\n",
    "\n",
    "This model was trained on the [BookSum dataset](https://arxiv.org/abs/2105.08209) which contains \"plays, short stories, and novels\" with expired copyright, and the aim of the training was to produce valuable summaries of the given documents.\n",
    "\n",
    "I will conduct transfer learning to remove the head of this model which is specifically focused on books, and train on the new task of summarising **privacy policies**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "model_checkpoint_name = \"pszemraj/led-large-book-summary\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_checkpoint_name)\n",
    "tokeniser.model_max_length, tokeniser.model_max_length * 4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:02.850129500Z",
     "start_time": "2023-12-27T23:25:02.666127200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-02T00:49:27.917533Z",
     "iopub.execute_input": "2024-01-02T00:49:27.917801Z",
     "iopub.status.idle": "2024-01-02T00:49:28.133189Z",
     "shell.execute_reply.started": "2024-01-02T00:49:27.917778Z",
     "shell.execute_reply": "2024-01-02T00:49:28.132157Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": [
    {
     "execution_count": 27,
     "output_type": "execute_result",
     "data": {
      "text/plain": "(16384, 65536)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above has taken the \"tokeniser\" from the model.\n",
    "\n",
    "Transformer models only take numerical inputs, thus, it's essential to get a numerical representation of the input.\n",
    "\n",
    "The tokeniser is responsible for turning sentences into a series of numbers, called \"tokens\".\n",
    "\n",
    "Tokenisers also use special characters, often to indicate the start and end of sentences or words.\n",
    "\n",
    "All this information helps a NLP neural network form an understanding of the input.\n",
    "\n",
    "The below output shows the tokens used to encode the given test string, with `<s>` being used to indicate the start of a sequence, and each subsequent token beginning with `Ġ`, indicating the start of a new token.\n",
    "\n",
    "Note that `tokeniser` was split into two tokens `token` and `iser`, this is important for tokenisers to be able to re-use tokens wherever possible - `iser` can be used as the suffix for many different words.\n",
    "\n",
    "An attention mask can tell the tokenizer not to pay attention to certain tokens."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "test_string = tokeniser(\"This is a test string to test out the tokeniser\")\n",
    "print(test_string)\n",
    "tokeniser.convert_ids_to_tokens(test_string.input_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:02.865783Z",
     "start_time": "2023-12-27T23:25:02.853128700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-02T00:49:28.134230Z",
     "iopub.execute_input": "2024-01-02T00:49:28.134511Z",
     "iopub.status.idle": "2024-01-02T00:49:28.141856Z",
     "shell.execute_reply.started": "2024-01-02T00:49:28.134489Z",
     "shell.execute_reply": "2024-01-02T00:49:28.141015Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'input_ids': [0, 713, 16, 10, 1296, 6755, 7, 1296, 66, 5, 19233, 5999, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
     "output_type": "stream"
    },
    {
     "execution_count": 28,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['<s>',\n 'This',\n 'Ġis',\n 'Ġa',\n 'Ġtest',\n 'Ġstring',\n 'Ġto',\n 'Ġtest',\n 'Ġout',\n 'Ġthe',\n 'Ġtoken',\n 'iser',\n '</s>']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, recall that the maximum token size for the model was 16384, or about 65536 words.\n",
    "\n",
    "The majority of the scraped documents still exceed this amount, unfortunately, since no bigger summarisation model exists, the input text will need to be truncated down to the maximum token size, which **will have an impact on the model's ability to learn** but there is no other option.\n",
    "\n",
    "Furthermore, the tokenizer will output may different outputs, one of which being `input_ids`, the numerical IDs of the tokens in the tokenised text.\n",
    "\n",
    "The `input_ids` will need to be assigned to the `labels` key of the tokenised input document. This prepares the tokenised input for a task where the model needs to predict the summary, which is how the model will learn.\n",
    "\n",
    "Below defines and runs a function which will truncate the input document, **but not the output** and assign the labels and input id's as necessary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def truncate_input_tokens(input):\n",
    "    truncated_input = tokeniser(\n",
    "        input[\"document\"],\n",
    "        max_length = 16384,\n",
    "        truncation = True\n",
    "    )\n",
    "    labels = tokeniser( # don't truncate output\n",
    "        input[\"summary\"],\n",
    "        truncation = False,\n",
    "    )\n",
    "    truncated_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return truncated_input\n",
    "\n",
    "# By passing `batched = true` into the map function, more than one item is applied to the function at a time.\n",
    "tokenised_dataset = dataset.map(truncate_input_tokens, batched = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:07.763873200Z",
     "start_time": "2023-12-27T23:25:02.868783400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-02T00:49:28.142992Z",
     "iopub.execute_input": "2024-01-02T00:49:28.143282Z",
     "iopub.status.idle": "2024-01-02T00:49:41.743446Z",
     "shell.execute_reply.started": "2024-01-02T00:49:28.143259Z",
     "shell.execute_reply": "2024-01-02T00:49:41.742597Z"
    },
    "trusted": true
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61a3244bfbf843c7b00e07ba40a7944b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b91b7f20ed104a4dbffa47edb8eddea4"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below now looks at the new dataset.\n",
    "\n",
    "We can see new features `input_ids`, `attention_mask` and `labels`\n",
    "\n",
    "The lengths of these inputs are also visible, both at 16384 (the maximum) tokens for the first input, and the biggest.\n",
    "\n",
    "The smallest input however, is just 694 tokens, suggesting some documents stayed well under the maximum."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(tokenised_dataset)\n",
    "\n",
    "length_of_first_item = len(tokenised_dataset['train'][0]['input_ids'])\n",
    "print(f'The length of the first terms in the train dataset is {length_of_first_item} tokens')\n",
    "\n",
    "length_of_longest_document = len(max(tokenised_dataset['train'], key=lambda x: len(x['input_ids']))['input_ids'])\n",
    "print(f'Length of the longest terms in the train dataset is {length_of_longest_document} tokens')\n",
    "\n",
    "length_of_longest_document = len(min(tokenised_dataset['train'], key=lambda x: len(x['input_ids']))['input_ids'])\n",
    "print(f'Length of the smallest terms in the train dataset is {length_of_longest_document} tokens')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:12.004420400Z",
     "start_time": "2023-12-27T23:25:07.760854Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-02T00:49:41.744657Z",
     "iopub.execute_input": "2024-01-02T00:49:41.744981Z",
     "iopub.status.idle": "2024-01-02T00:49:51.009957Z",
     "shell.execute_reply.started": "2024-01-02T00:49:41.744955Z",
     "shell.execute_reply": "2024-01-02T00:49:51.009010Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "text": "DatasetDict({\n    train: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 258\n    })\n    test: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 29\n    })\n})\nThe length of the first terms in the train dataset is 16384 tokens\nLength of the longest terms in the train dataset is 16384 tokens\nLength of the smallest terms in the train dataset is 694 tokens\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining Evaluation Techniques"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In Evaluation of summarisation tasks in NLP, the ROUGE metric is most often used.\n",
    "\n",
    "To evaluate the model, I will use the `Hugging Face evaluate` library, as well as the `rouge-score` library to produce the rouge score for the model's summaries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:20.164335900Z",
     "start_time": "2023-12-27T23:25:12.005422Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-02T00:49:51.013160Z",
     "iopub.execute_input": "2024-01-02T00:49:51.013485Z",
     "iopub.status.idle": "2024-01-02T00:50:14.750824Z",
     "shell.execute_reply.started": "2024-01-02T00:49:51.013458Z",
     "shell.execute_reply": "2024-01-02T00:50:14.749669Z"
    },
    "trusted": true
   },
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.12.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.24.3)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To calculate the ROUGE metric on predictions, we need the **decoded** predictions and the **decoded** labels, as they are currently *tokenised* (labels simply refers to the expected values).\n",
    "\n",
    "Thus, we need to decode the input tokens back into their English representations.\n",
    "\n",
    "The `batch-decode` function does this, with the parameter `skip_special_tokens` specifying to not handle any special tokens such as `<s>` indicating the start of a sequence. \n",
    "\n",
    "The function below will decode and find the ROUGE score."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def decode_and_find_rouge(result):\n",
    "    predictions,labels = result # extract predictions and labels from the passed in result.\n",
    "    \n",
    "    # Decode by converting tokenised inputs back into English representations\n",
    "    decoded_predictions = tokeniser.batch_decode(predictions, skip_special_tokens=True) \n",
    "    decoded_labels = tokeniser.batch_decode(labels, skip_special_tokens = True)\n",
    "\n",
    "    rouge_score = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "\n",
    "    num_of_predictions = [np.count_nonzero(pred != tokeniser.pad_token_id) for pred in predictions]\n",
    "    rouge_score[\"gen_len\"] = np.mean(num_of_predictions)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in rouge_score.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:20.179299900Z",
     "start_time": "2023-12-27T23:25:20.167300500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-02T00:50:14.753988Z",
     "iopub.execute_input": "2024-01-02T00:50:14.754290Z",
     "iopub.status.idle": "2024-01-02T00:50:14.761210Z",
     "shell.execute_reply.started": "2024-01-02T00:50:14.754262Z",
     "shell.execute_reply": "2024-01-02T00:50:14.760289Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, the model hyper-paramters need to be defined.\n",
    "\n",
    "This is a very expensive model on memory and it is essential to avoid exceeding the GPU's VRAM, thus the hyper-parameters are configured as such:\n",
    "\n",
    "1) The batch size will be set to just 1 to reduce the amount of data stored in memory.\n",
    "\n",
    "2) 16-bit floating point precision will be used, rather than 32-bit.\n",
    "\n",
    "3) The optimiser \"adafactor\" is used over the industry standard of \"Adam\". As a result, the model converges slower but also uses less memory.\n",
    "\n",
    "4) \"Gradient Checkpointing\" is used, instructing the model to forget the majority of forward-pass activations and instead recompute them on demand during the backward pass, saving only the \"most important\" activations. Significantly slower training time, but also saves a lot of VRAM.\n",
    "\n",
    "5) \"Gradient Accumulation\" is used simulate a \"larger\" effective batch size. Instead of updating the model's parameters after processing eachbatch, the gradients are accumulated over 2 batches before performing a single update."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint_name)\n",
    "\n",
    "# Creating a data collator, which will form the batches which will be fed to the model.\n",
    "# It will also conduct padding if necessary to get all inputs of equal length.\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokeniser, model=model_checkpoint_name)\n",
    "\n",
    "models_arguments = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"TOS_4_Epoch_Model\",\n",
    "    evaluation_strategy=\"epoch\", # Run evaluation function on each epoch\n",
    "    learning_rate=2e-5, # learning rate hyperparameter set to 0.00002\n",
    "    per_device_train_batch_size= 1, # split into batches of 1 for training\n",
    "    per_device_eval_batch_size=1, # split to batches of 1 for evaluation\n",
    "    weight_decay=0.01, # Utilises L2 regularization in an attempt to prevent overfitting\n",
    "    save_total_limit=3, # save 3 checkpoints only and delete older checkpoints (Kept using all RAM without)\n",
    "    num_train_epochs=4, # train for 3 epochs\n",
    "    predict_with_generate=True, # Generate summaries for each input ; essential for summarisation tasks\n",
    "    fp16=True, # use 16 bit floating point - reduced memory usage\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\"\n",
    ")\n",
    "\n",
    "# Collect the previously defined trainer parameters, such as the evaluation technique, tokeniser and datasets.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=models_arguments,\n",
    "    train_dataset=tokenised_dataset[\"train\"],\n",
    "    eval_dataset=tokenised_dataset[\"test\"],\n",
    "    tokenizer=tokeniser,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=decode_and_find_rouge,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:24.378548100Z",
     "start_time": "2023-12-27T23:25:20.183300400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-02T00:50:14.762574Z",
     "iopub.execute_input": "2024-01-02T00:50:14.762928Z",
     "iopub.status.idle": "2024-01-02T00:50:19.598666Z",
     "shell.execute_reply.started": "2024-01-02T00:50:14.762904Z",
     "shell.execute_reply": "2024-01-02T00:50:19.597908Z"
    },
    "trusted": true
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, the model can be ran using the `train()` function on the trainer.\n",
    "\n",
    "After training, the model will be saved in a file called \"trained_model\".\n",
    " - The size of the model is ~1.7GB"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Uncomment below line if using Kaggle.\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"TOS_4_Epoch_Model\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:26:16.985454Z",
     "start_time": "2023-12-27T23:25:42.912321500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-01-02T00:50:19.599873Z",
     "iopub.execute_input": "2024-01-02T00:50:19.600152Z",
     "iopub.status.idle": "2024-01-02T03:40:14.523172Z",
     "shell.execute_reply.started": "2024-01-02T00:50:19.600127Z",
     "shell.execute_reply": "2024-01-02T03:40:14.522090Z"
    },
    "trusted": true
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a LEDTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='516' max='516' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [516/516 2:49:01, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.693679</td>\n      <td>0.480600</td>\n      <td>0.286600</td>\n      <td>0.250900</td>\n      <td>0.250300</td>\n      <td>297.379300</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.593070</td>\n      <td>0.458700</td>\n      <td>0.298000</td>\n      <td>0.271000</td>\n      <td>0.270800</td>\n      <td>156.689700</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.552057</td>\n      <td>0.447100</td>\n      <td>0.275600</td>\n      <td>0.277100</td>\n      <td>0.275600</td>\n      <td>158.275900</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.642000</td>\n      <td>0.547224</td>\n      <td>0.440600</td>\n      <td>0.262800</td>\n      <td>0.255000</td>\n      <td>0.254500</td>\n      <td>139.655200</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating the model\n",
    "\n",
    "First, we can observe the output of the model on some examples, to see how the model is handling terms and conditions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel,LEDTokenizer, LEDForConditionalGeneration\n",
    "\n",
    "trained_tokeniser = LEDTokenizer.from_pretrained(\"/kaggle/working/TOS_4_Epoch_Model\")\n",
    "\n",
    "trained_model = LEDForConditionalGeneration.from_pretrained(\"/kaggle/working/TOS_4_Epoch_Model\")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=trained_model,tokenizer=trained_tokeniser)\n",
    "\n",
    "\n",
    "example = dataset[\"test\"][\"document\"][0]\n",
    "\n",
    "tokenised_example = trained_tokeniser(example,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "\n",
    "outputs = trained_model.generate(tokenised_example)\n",
    "trained_tokeniser.decode(outputs[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-02T03:45:08.214420Z",
     "iopub.execute_input": "2024-01-02T03:45:08.214812Z",
     "iopub.status.idle": "2024-01-02T03:48:29.292740Z",
     "shell.execute_reply.started": "2024-01-02T03:45:08.214781Z",
     "shell.execute_reply": "2024-01-02T03:48:29.291470Z"
    },
    "trusted": true
   },
   "execution_count": 38,
   "outputs": [
    {
     "execution_count": 38,
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"The service is provided 'as is' and to be used at the users' sole risk. The service provider makes no warranty regarding uninterrupted, timely, secure or error-free service. Failure to enforce any provision of the Terms of Service does not constitute a waiver of such provision. This service assumes no liability for any losses or damages resulting from any matter relating to the service. You agree to defend, indemnify, and hold the service harmless in case of a claim related to your use of the service\""
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "second_example = dataset[\"test\"][\"document\"][1]\n",
    "\n",
    "tokenised_example_2 = trained_tokeniser(second_example,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "outputs_2 = trained_model.generate(tokenised_example_2)\n",
    "trained_tokeniser.decode(outputs_2[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-02T03:48:29.294972Z",
     "iopub.execute_input": "2024-01-02T03:48:29.295367Z",
     "iopub.status.idle": "2024-01-02T03:52:05.355950Z",
     "shell.execute_reply.started": "2024-01-02T03:48:29.295331Z",
     "shell.execute_reply": "2024-01-02T03:52:05.354720Z"
    },
    "trusted": true
   },
   "execution_count": 39,
   "outputs": [
    {
     "execution_count": 39,
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"The service is provided 'as is' and to be used at the users' sole risk. The service provider makes no warranty regarding uninterrupted, timely, secure or error-free service. This service assumes no liability for any losses or damages resulting from any matter relating to the service. You waive your right to a class action.. Your account can be deleted without prior notice and without a reason.  Terms may be changed any time at their discretion, without notice to you. Instead of asking directly, this Service will assume your consent merely from your usage.. There is a date of the last update of the agreements (January 2020)\""
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "lorem_example = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\"\n",
    "\n",
    "tokenised_example_lorem = trained_tokeniser(lorem_example,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "outputs_lorem = trained_model.generate(tokenised_example_lorem)\n",
    "trained_tokeniser.decode(outputs_lorem[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-02T03:52:05.357515Z",
     "iopub.execute_input": "2024-01-02T03:52:05.358021Z",
     "iopub.status.idle": "2024-01-02T03:52:17.125823Z",
     "shell.execute_reply.started": "2024-01-02T03:52:05.357993Z",
     "shell.execute_reply": "2024-01-02T03:52:17.124360Z"
    },
    "trusted": true
   },
   "execution_count": 40,
   "outputs": [
    {
     "execution_count": 40,
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"The service is provided 'as is' and to be used at your sole risk. You are responsible for any risks, damages, or losses that may incur by downloading materials. This service does not guarantee that it or the products obtained through it meet your expectations or requirements.\""
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the surface these summaries look sufficient.\n",
    "\n",
    "We can look at the ROUGE scores of a test set example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "example_document = dataset[\"test\"][\"document\"][5]\n",
    "ground_truth_summary = dataset[\"test\"][\"summary\"][5]\n",
    "\n",
    "tokenised_document = trained_tokeniser(example_document,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "tokenised_ground_truth = trained_tokeniser(ground_truth_summary,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "output_summary = trained_model.generate(tokenised_document)\n",
    "\n",
    "decoded_summary = trained_tokeniser.decode(output_summary[0], skip_special_tokens=True)\n",
    "\n",
    "# Rouge metric expects array.\n",
    "truth = [ground_truth_summary]\n",
    "summary = [decoded_summary]\n",
    "\n",
    "rouge_score = rouge.compute(predictions=truth, references=summary)\n",
    "\n",
    "print(rouge_score)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-02T03:52:17.129522Z",
     "iopub.execute_input": "2024-01-02T03:52:17.129834Z",
     "iopub.status.idle": "2024-01-02T03:53:13.066732Z",
     "shell.execute_reply.started": "2024-01-02T03:52:17.129808Z",
     "shell.execute_reply": "2024-01-02T03:53:13.065280Z"
    },
    "trusted": true
   },
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'rouge1': 0.49402390438247007, 'rouge2': 0.24096385542168675, 'rougeL': 0.26294820717131473, 'rougeLsum': 0.26294820717131473}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now take the ROUGE score across the entire test set\n",
    "\n",
    "This will take a while, due to needing to create summaries for the entire test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "summaries = []\n",
    "truths = []\n",
    "count = 1\n",
    "\n",
    "for item in dataset[\"test\"]:\n",
    "    print(f'Processing document {count} of {len(dataset[\"test\"])}')\n",
    "    document = item[\"document\"]\n",
    "    ground_truth_summary = item[\"summary\"]\n",
    "    \n",
    "    tokenised_document = trained_tokeniser(document,return_tensors=\"pt\",truncation=True).input_ids\n",
    "    generated_summary = trained_model.generate(tokenised_document)\n",
    "    decoded_summary = trained_tokeniser.decode(generated_summary[0], skip_special_tokens=True)\n",
    "    \n",
    "    summaries.append(decoded_summary)\n",
    "    truths.append(ground_truth_summary)\n",
    "    count += 1"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "rouge_score = rouge.compute(predictions=summaries,references=truths)\n",
    "print(rouge_score)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-02T05:36:25.528294Z",
     "iopub.execute_input": "2024-01-02T05:36:25.529302Z",
     "iopub.status.idle": "2024-01-02T05:36:26.574642Z",
     "shell.execute_reply.started": "2024-01-02T05:36:25.529266Z",
     "shell.execute_reply": "2024-01-02T05:36:26.573523Z"
    },
    "trusted": true
   },
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'rouge1': 0.45041833108246265, 'rouge2': 0.2748145311460465, 'rougeL': 0.2588148235927864, 'rougeLsum': 0.2582850488220999}\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
