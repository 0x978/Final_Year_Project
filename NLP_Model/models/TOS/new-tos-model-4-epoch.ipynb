{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7293442,
     "sourceType": "datasetVersion",
     "datasetId": 4230174
    },
    {
     "sourceId": 7293534,
     "sourceType": "datasetVersion",
     "datasetId": 4230224
    },
    {
     "sourceId": 7300111,
     "sourceType": "datasetVersion",
     "datasetId": 4234911
    },
    {
     "sourceId": 7320296,
     "sourceType": "datasetVersion",
     "datasetId": 4248138
    },
    {
     "sourceId": 7326507,
     "sourceType": "datasetVersion",
     "datasetId": 4252461
    },
    {
     "sourceId": 7333022,
     "sourceType": "datasetVersion",
     "datasetId": 4256963
    },
    {
     "sourceId": 7392147,
     "sourceType": "datasetVersion",
     "datasetId": 4297367
    }
   ],
   "dockerImageVersionId": 30636,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install required libraries and imports."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U transformers\n",
    "!pip3 install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -U datasets\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "# Allocate maximum CUDA memory reserve in an attempt to prevent CUDA out of memory errors\n",
    "# Reserve is simply the reserved memory, not the in-use memory.\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:21.193654300Z",
     "start_time": "2024-01-13T01:36:14.536744900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:43:31.668063Z",
     "iopub.execute_input": "2024-01-14T17:43:31.668374Z",
     "iopub.status.idle": "2024-01-14T17:44:13.254349Z",
     "shell.execute_reply.started": "2024-01-14T17:43:31.668346Z",
     "shell.execute_reply": "2024-01-14T17:44:13.253487Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nLooking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting datasets\n  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/ec/93/454ada0d1b289a0f4a86ac88dbdeab54921becabac45da3da787d136628f/datasets-2.16.1-py3-none-any.whl.metadata\n  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.12.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nCollecting pyarrow-hotfix (from datasets)\n  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nCollecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets)\n  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m507.1/507.1 kB\u001B[0m \u001B[31m13.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.4/166.4 kB\u001B[0m \u001B[31m15.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: pyarrow-hotfix, fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.12.2\n    Uninstalling fsspec-2023.12.2:\n      Successfully uninstalled fsspec-2023.12.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed datasets-2.16.1 fsspec-2023.10.0 pyarrow-hotfix-0.6\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tell PyTorch to use GPU wherever possible"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:14:28.292515800Z",
     "start_time": "2024-01-13T02:14:28.282516500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:44:13.255969Z",
     "iopub.execute_input": "2024-01-14T17:44:13.256421Z",
     "iopub.status.idle": "2024-01-14T17:44:16.363206Z",
     "shell.execute_reply.started": "2024-01-14T17:44:13.256395Z",
     "shell.execute_reply": "2024-01-14T17:44:16.362250Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, the dataset and checkpoint needs to be initialised."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset location\n",
    "Dataset location depends on where notebook is running, for ease I set it up to just uncomment line below depending on location the notebook is running as I run the notebook in a lot of locations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_location = \"/kaggle/input/new-datasets/Terms_dataset.jsonl\" # Kaggle\n",
    "\n",
    "#dataset_location = \"Privacy_Policy_dataset.jsonl\" # Local / Google Colab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:21.238664900Z",
     "start_time": "2024-01-13T01:36:21.195654200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:44:16.364700Z",
     "iopub.execute_input": "2024-01-14T17:44:16.365788Z",
     "iopub.status.idle": "2024-01-14T17:44:16.370212Z",
     "shell.execute_reply.started": "2024-01-14T17:44:16.365749Z",
     "shell.execute_reply": "2024-01-14T17:44:16.369228Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialise dataset\n",
    "To initialise the dataset I use the \"dataset\" library from python.\n",
    "\n",
    "I split the dataset into three sets:\n",
    "- Training set - The data shown to the model during training\n",
    "- Validation - The data shown to the model to calculate loss on backward pass\n",
    "- Test - Reserved strictly for after the model is trained, used to evaluate the model on a completely unseen set\n",
    "\n",
    "However, the \"datasets\" library doesn't offer the possibility to split into three sets so I use a workaround sourced from: [This stackoverflow post](https://stackoverflow.com/questions/76001128/splitting-dataset-into-train-test-and-validation-using-huggingface-datasets-fun)\n",
    "\n",
    "It works by first splitting the data set into a train set (80%) and a validation set (20%).\n",
    "\n",
    "It then splits this validation set into a train set and validation set of 50% each, resulting in two sets of 10% each.\n",
    "\n",
    "A final dataset consisting of a train, test and validation set is then built using these split datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"json\", data_files=dataset_location,split='train')\n",
    "\n",
    "dataset = dataset.shuffle(seed=2424)\n",
    "\n",
    "test_valid_split_dataset = dataset.train_test_split(test_size=0.2, shuffle=False)\n",
    "\n",
    "test_split = test_valid_split_dataset['test'].train_test_split(test_size=0.5, shuffle = False)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': test_valid_split_dataset['train'],\n",
    "    'test': test_split['test'],\n",
    "    'valid': test_split['train']})\n",
    "\n",
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:21.948222400Z",
     "start_time": "2024-01-13T01:36:21.212681500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:44:16.372515Z",
     "iopub.execute_input": "2024-01-14T17:44:16.372814Z",
     "iopub.status.idle": "2024-01-14T17:44:17.109374Z",
     "shell.execute_reply.started": "2024-01-14T17:44:16.372789Z",
     "shell.execute_reply": "2024-01-14T17:44:17.108310Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f3d6a19254c4944a910ad48c46831b8"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "DatasetDict({\n    train: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 417\n    })\n    test: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 53\n    })\n    valid: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 52\n    })\n})\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reviewing the dataset\n",
    "Next I want to see the properties of the dataset, to understand what i'm working with.\n",
    "\n",
    "For training a summarisation model knowing the length of the collected documents is crucial.\n",
    "\n",
    "The largest summarisation base model is only capable of processing 16384 tokens - higher token limits is a limitation in NLP as a whole.\n",
    "\n",
    "Roughly, [One token is equal to about four English characters](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)\n",
    "\n",
    "This gives roughly 65,536 characters which the model will be able to parse and create a summary for, and anything which exceeds this number needs to be truncated down to the 16384 token limit.\n",
    "\n",
    "This unfortunately means on some documents, some detail will be missing.\n",
    "\n",
    "However, as visible below, the average document in the train set is just 29,962 characters, considerably under the maximum token limit, thus, for most items in the dataset this isn't a problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "length_of_first_item = len(dataset['train'][0]['document'])\n",
    "print(f'The length of the first document in the train dataset is {length_of_first_item} characters')\n",
    "\n",
    "length_of_longest_document = len(max(dataset['train'], key=lambda x: len(x['document']))['document'])\n",
    "print(f'Length of the longest document in the train dataset is {length_of_longest_document} characters')\n",
    "\n",
    "length_of_shortest_document = len(min(dataset['train'], key=lambda x: len(x['document']))['document'])\n",
    "print(f'Length of the shortest document in the train dataset is {length_of_shortest_document} characters')\n",
    "\n",
    "total_char_count = sum(map(len, dataset['train']['document']))\n",
    "avg_char_count = round(total_char_count / len(dataset['train']['document']))\n",
    "\n",
    "print(f'The average character count in the dataset is: {avg_char_count}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.059222300Z",
     "start_time": "2024-01-13T01:36:21.951221600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:44:17.110641Z",
     "iopub.execute_input": "2024-01-14T17:44:17.110964Z",
     "iopub.status.idle": "2024-01-14T17:44:17.250946Z",
     "shell.execute_reply.started": "2024-01-14T17:44:17.110928Z",
     "shell.execute_reply": "2024-01-14T17:44:17.249971Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "The length of the first document in the train dataset is 111553 characters\nLength of the longest document in the train dataset is 247686 characters\nLength of the shortest document in the train dataset is 1472 characters\nThe average character count in the dataset is: 29962\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Base model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I will be utilising transfer learning to train a model.\n",
    "\n",
    "This takes the base of a model trained on some other task but in a similar domain (e.g. summarising books), removes the head of the model which is more specialised (e.g. contains information specific to books), while retaining useful information about the English language. The model is then trained on a new specific task, in my case, summarising terms and conditions or privacy policies, utilising its pre-existing knowledge of the English language.\n",
    "\n",
    "This significantly reduces training time and resources required for training such that I can stay within the final year project deadlines.\n",
    "\n",
    "The model I will be using as a base is the [led-large-book-summary](https://huggingface.co/pszemraj/led-large-book-summary). This model utilises the Longformer Encoder-Decoder (LED) model as it's base, and was trained further to summarise long-form text such as novels, plays and stories from the [BookSum dataset](https://arxiv.org/abs/2105.08209)\n",
    "\n",
    " Below I initialise the tokeniser for this model through the [Hugging Face](https://huggingface.co/models) library, which offer a variety of base models for transfer-learning. \n",
    "  - This base model is ~1.5 GB \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_name = \"pszemraj/led-large-book-summary\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(base_model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.618471300Z",
     "start_time": "2024-01-13T01:36:22.061221600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:44:17.252445Z",
     "iopub.execute_input": "2024-01-14T17:44:17.252851Z",
     "iopub.status.idle": "2024-01-14T17:44:20.800169Z",
     "shell.execute_reply.started": "2024-01-14T17:44:17.252814Z",
     "shell.execute_reply": "2024-01-14T17:44:20.799201Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "909a2255d29240efa7af760a0c52fbd4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "271c6c18f62e4a0eade72e5d130cff33"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c45a64570644453adb38f497e87ee13"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ee9dbcbb23148e5aca7574a196ab00c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0383f929518b410a9eabe105449579c2"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformer models *only* take numerical inputs, thus, a tokeniser is responsible for transforming the input text into its numerical representation.\n",
    "\n",
    "It does this by splitting text into \"tokens\" which are small groups of characters.\n",
    "\n",
    "Tokenisers also use special characters, often indicating the start and the end of sequences and words.\n",
    "\n",
    "Below I display an example by tokenising a simple string using the tokeniser for the \"led-large-book-summary\" model\n",
    "\n",
    "The `input_ids` represents the tokenised input, `attention_mask` is tells the model to ignore tokens if the equivalent index in the attention mask array is zero.\n",
    "\n",
    "Note when converting the string back to English, we can see`<s>` being used to indicate the start of a sequence, and each word beginning with `Ġ`.\n",
    " - This changes depending on the base model, but the Hugging Face library picks out the right tokeniser for the base model.\n",
    "\n",
    "Also, words can be split into two tokens if it is deemed useful, below `tokeniser` is split into two tokens `token` and `iser`, as the tokens `iser` and `token` could later be re-used with other words, saving tokens."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "test_string = tokeniser(\"This is a test string to test out the tokeniser\")\n",
    "print(test_string)\n",
    "tokeniser.convert_ids_to_tokens(test_string.input_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.629472300Z",
     "start_time": "2024-01-13T01:36:22.614472800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:44:20.801707Z",
     "iopub.execute_input": "2024-01-14T17:44:20.802933Z",
     "iopub.status.idle": "2024-01-14T17:44:20.815499Z",
     "shell.execute_reply.started": "2024-01-14T17:44:20.802869Z",
     "shell.execute_reply": "2024-01-14T17:44:20.814447Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'input_ids': [0, 713, 16, 10, 1296, 6755, 7, 1296, 66, 5, 19233, 5999, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
     "output_type": "stream"
    },
    {
     "execution_count": 7,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['<s>',\n 'This',\n 'Ġis',\n 'Ġa',\n 'Ġtest',\n 'Ġstring',\n 'Ġto',\n 'Ġtest',\n 'Ġout',\n 'Ġthe',\n 'Ġtoken',\n 'iser',\n '</s>']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, a function is needed which will tokenise the text.\n",
    "\n",
    "Here, the maximum length can be defined as 16384 tokens and the tokeniser will be responsible for ensuring any text exceeding this is truncated.\n",
    "\n",
    "In the dataset, the \"document\" column contains the input document (privacy policy or Terms of Service)\n",
    "\n",
    "The \"summary\" column contains the ground truth summary for the matching document.\n",
    " - This doesn't need to be truncated, as they are all <500 tokens\n",
    "\n",
    "Furthermore, this function assigns the tokenised ground truth summaries `input_id`'s to the \"labels\" property of the tokenised documents.\n",
    " - This is the format required for training a summarisaiton model in pytorch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenise_truncate_dataset(input):\n",
    "    truncated_input = tokeniser(\n",
    "        input[\"document\"],\n",
    "        max_length = 16384,\n",
    "        truncation = True\n",
    "    )\n",
    "    labels = tokeniser( # don't truncate labels\n",
    "        input[\"summary\"],\n",
    "        truncation = False,\n",
    "    )\n",
    "    truncated_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return truncated_input\n",
    "# By passing `batched = true` into the map function, more than one item is applied to the function at a time.\n",
    "tokenised_dataset = dataset.map(tokenise_truncate_dataset, batched = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.723472600Z",
     "start_time": "2024-01-13T01:36:22.631472300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:44:20.817251Z",
     "iopub.execute_input": "2024-01-14T17:44:20.817524Z",
     "iopub.status.idle": "2024-01-14T17:44:27.170297Z",
     "shell.execute_reply.started": "2024-01-14T17:44:20.817501Z",
     "shell.execute_reply": "2024-01-14T17:44:27.169371Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/417 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ebb7feddb3b4a9f914b92218cd40c67"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/53 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4607f0ed50d84de4a558f4fda1a6abf6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/52 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a1625878238462c8f0fe47d97ce7238"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The properties of the dataset can now be viewed, as expected, there is a train, test and validation dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "tokenised_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.740471900Z",
     "start_time": "2024-01-13T01:36:22.725471800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:44:27.171744Z",
     "iopub.execute_input": "2024-01-14T17:44:27.172212Z",
     "iopub.status.idle": "2024-01-14T17:44:27.178356Z",
     "shell.execute_reply.started": "2024-01-14T17:44:27.172176Z",
     "shell.execute_reply": "2024-01-14T17:44:27.177451Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 417\n    })\n    test: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 53\n    })\n    valid: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 52\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, the \"summary\" and \"document\" dataset are no longer needed, as we have their tokenised equivalents - \"input_ids\" and \"labels\".\n",
    "\n",
    "Thus, these can be removed.\n",
    "\n",
    "Furthermore, the dataset needs to be set to return pytorch tensors, in order to be able to be trained in pytorch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "tokenised_dataset = tokenised_dataset.remove_columns([\"summary\",\"document\"])\n",
    "tokenised_dataset.set_format(\"torch\")\n",
    "tokenised_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.783937100Z",
     "start_time": "2024-01-13T01:36:22.741474200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:44:27.181558Z",
     "iopub.execute_input": "2024-01-14T17:44:27.181837Z",
     "iopub.status.idle": "2024-01-14T17:44:27.209655Z",
     "shell.execute_reply.started": "2024-01-14T17:44:27.181813Z",
     "shell.execute_reply": "2024-01-14T17:44:27.208631Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "execution_count": 10,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 417\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 53\n    })\n    valid: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 52\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, a data collator needs to be defined.\n",
    "\n",
    "(Below Information sourced from: https://huggingface.co/docs/transformers/main_classes/data_collator)\n",
    "\n",
    "This is responsible for constructing batches and applying pre-processing such as padding to ensure all inputs are of the same size.\n",
    "\n",
    "The Hugging Face library provides a function for sourcing a data collator with padding."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokeniser, model= base_model_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:42:02.436350100Z",
     "start_time": "2024-01-13T01:42:01.202269900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:44:27.210993Z",
     "iopub.execute_input": "2024-01-14T17:44:27.211612Z",
     "iopub.status.idle": "2024-01-14T17:44:37.330631Z",
     "shell.execute_reply.started": "2024-01-14T17:44:27.211576Z",
     "shell.execute_reply": "2024-01-14T17:44:37.329859Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "First, the base model needs to be defined.\n",
    "\n",
    "Next, an optimiser needs to be defined.\n",
    "\n",
    "The standard optimiser to use is `adamW`, but again, due to VRAM limitations, a less memory-intensive optimiser will be used called \"adafactor\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "from transformers.optimization import Adafactor\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:01:43.238089600Z",
     "start_time": "2024-01-13T02:01:40.995123800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:44:37.331704Z",
     "iopub.execute_input": "2024-01-14T17:44:37.332278Z",
     "iopub.status.idle": "2024-01-14T17:44:45.905779Z",
     "shell.execute_reply.started": "2024-01-14T17:44:37.332250Z",
     "shell.execute_reply": "2024-01-14T17:44:45.904837Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d74f4829e1c4425d9328713e9ee7ecd8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "add5e5d977d84ee7aac2d87a90fd88dd"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, ROUGE needs to be defined for the evaluation loop, and a function needs to be defined which will compute the rouge scores.\n",
    "\n",
    "The rouge score metric is imported from the `rouge_score` python library, which will calculate and return the following metrics\n",
    " \n",
    " ### ROUGE-1\n",
    " ROUGE-1 is the overlap of words between the produced summary and ground truth summary.\n",
    " $$\\frac{overlapping \\space words}{total \\space words}$$\n",
    " \n",
    " ### ROUGE-2\n",
    " ROUGE-2 is the overlap of bi-grams (pairs of words)\n",
    " \n",
    " $$\\frac{overlapping \\space pairs}{total \\space number \\space of \\space bi-grams}$$ \n",
    " \n",
    " ### ROUGE-L\n",
    " Source partially from (https://en.wikipedia.org/wiki/Longest_common_subsequence)\n",
    " \n",
    " ROUGE-L is based on the idea of a *longest common subsequence* - the longest common sequence of words between two texts.\n",
    " \n",
    " A subsequence does not necessarily have to be *consecutive*, letters can be skipped to make a subsequence.\n",
    " \n",
    " Consider strings \"ABCD\" and \"ACBAD\" - the longest common subseqeunces sequences are ABD and ACD.\n",
    " \n",
    " $$\\frac{Longest \\space common \\space subsequence}{total \\space words}$$ \n",
    " \n",
    " Such that for the above example, if we consider \"ABCD\" the produced text and \"ACBAD\" the ground truth, ROUGE-L is equal to $ \\frac{3}{5} = 0.6$\n",
    " \n",
    " ### ROUGE-L-SUM\n",
    " sourced from https://dev.to/aws-builders/mastering-rouge-matrix-your-guide-to-large-language-model-evaluation-for-summarization-with-examples-jjg\n",
    " \n",
    "This is similar to ROUGE-L, but instead compares this at a *sentence* level, calculating ROUGE-L for each sentence in the produced summary.\n",
    "\n",
    "This is a better measurement of accuracy in my use-case, as the produced summaries are split into sentences, where each sentence is a \"summary point\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def decode_and_find_rouge(result):\n",
    "    predictions,labels = result # extract predictions and labels from the passed in result.\n",
    "\n",
    "    # Decode by converting tokenised inputs back into English representations\n",
    "    decoded_predictions = tokeniser.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokeniser.batch_decode(labels, skip_special_tokens = True)\n",
    "    \n",
    "    # Calculate rouge scores by passing in predictions and ground truths\n",
    "    rouge_score = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "    \n",
    "    # Calculate the average generated length fo examples.\n",
    "    num_of_predictions = [np.count_nonzero(pred != tokeniser.pad_token_id) for pred in predictions]\n",
    "    rouge_score[\"gen_len\"] = np.mean(num_of_predictions)\n",
    "\n",
    "    # Round and return results.\n",
    "    return {k: round(v, 4) for k, v in rouge_score.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:18:36.724807300Z",
     "start_time": "2024-01-13T02:18:26.022709800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:44:45.907199Z",
     "iopub.execute_input": "2024-01-14T17:44:45.907602Z",
     "iopub.status.idle": "2024-01-14T17:45:20.123717Z",
     "shell.execute_reply.started": "2024-01-14T17:44:45.907551Z",
     "shell.execute_reply": "2024-01-14T17:45:20.122857Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Collecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.16.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.12.2)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.24.3)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=21deca0b10972b60fc2adf7b824f03382397aa43031a9829dc1fa1247bb8acd2\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82679f1a1dd144ed98d9a99469a41cc1"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, the model training loop needs to be defined:\n",
    "\n",
    "This is a very expensive model on memory and it is essential to avoid exceeding the GPU's VRAM, thus the hyper-parameters are configured as such:\n",
    "\n",
    "1) The batch size will be set to just 1 to reduce the amount of data stored in memory.\n",
    "\n",
    "2) 16-bit floating point precision will be used, rather than 32-bit.\n",
    "\n",
    "3) The optimiser \"adafactor\" is used over the industry standard of \"Adam\". As a result, the model converges slower but also uses less memory.\n",
    "\n",
    "4) \"Gradient Checkpointing\" is used, instructing the model to forget the majority of forward-pass activations and instead recompute them on demand during the backward pass, saving only the \"most important\" activations. Significantly slower training time, but also saves a lot of VRAM.\n",
    "\n",
    "5) \"Gradient Accumulation\" is used simulate a \"larger\" effective batch size. Instead of updating the model's parameters after processing eachbatch, the gradients are accumulated over 2 batches before performing a single update."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "models_arguments = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"new_terms_model_4_epoch\",\n",
    "    evaluation_strategy=\"epoch\", # Run evaluation function on each epoch\n",
    "    learning_rate=2e-5, # learning rate hyperparameter set to 0.00002\n",
    "    per_device_train_batch_size= 1, # split into batches of 1 for training\n",
    "    per_device_eval_batch_size=1, # split to batches of 1 for evaluation\n",
    "    weight_decay=0.01, # Utilises L2 regularization in an attempt to prevent overfitting\n",
    "    save_total_limit=3, # save 3 checkpoints only and delete older checkpoints (Kept using all RAM without)\n",
    "    num_train_epochs=4, # train for 4 epochs\n",
    "    predict_with_generate=True, # Generate summaries for each input ; essential for summarisation tasks\n",
    "    fp16=True, # use 16 bit floating point - reduced memory usage\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\"\n",
    ")\n",
    "\n",
    "# Collect the previously defined trainer parameters, such as the evaluation technique, tokeniser and datasets.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=base_model,\n",
    "    args=models_arguments,\n",
    "    train_dataset=tokenised_dataset[\"train\"],\n",
    "    eval_dataset=tokenised_dataset[\"valid\"],\n",
    "    tokenizer=tokeniser,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=decode_and_find_rouge,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "# Uncomment below line if using Kaggle.\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"new_terms_model_4_epoch\")"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-14T17:45:20.125237Z",
     "iopub.execute_input": "2024-01-14T17:45:20.125613Z",
     "iopub.status.idle": "2024-01-14T19:59:52.236631Z",
     "shell.execute_reply.started": "2024-01-14T17:45:20.125570Z",
     "shell.execute_reply": "2024-01-14T19:59:52.235433Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.2"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a LEDTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='832' max='832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [832/832 2:13:46, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>0.596928</td>\n      <td>0.401200</td>\n      <td>0.249500</td>\n      <td>0.250800</td>\n      <td>0.250600</td>\n      <td>114.846200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.741200</td>\n      <td>0.500441</td>\n      <td>0.475500</td>\n      <td>0.299200</td>\n      <td>0.262600</td>\n      <td>0.262900</td>\n      <td>158.250000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.741200</td>\n      <td>0.487602</td>\n      <td>0.487400</td>\n      <td>0.320200</td>\n      <td>0.267800</td>\n      <td>0.268000</td>\n      <td>170.173100</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating tokeniser\n",
    "\n",
    "First, I want to evaluate the tokeniser to see if a lot of text is being truncated and lost during the tokenisation process\n",
    "\n",
    "From the example below we can see some characters are lost, however, on review it appears that it is just removing excessive text that existed in the input text, a specific example is below:\n",
    "\n",
    "`For the purposes of the , is the \"data controller\".`\n",
    "\n",
    "became:\n",
    "\n",
    "`For the purposes of the, is the \"data controller\".`\n",
    "\n",
    "Note the absence of the extra space before the comma which existed in the un-tokenised data.\n",
    "\n",
    "also note the sentence appears to be missing a word - this is a side effect of webscraping, which missed a very minor amount of text while collecting the dataset, such as perhaps the website using a HTML element that wasn't expected..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel,LEDTokenizer, LEDForConditionalGeneration\n",
    "\n",
    "trained_tokeniser = LEDTokenizer.from_pretrained(\"/kaggle/working/new_terms_model_4_epoch\")\n",
    "doc = dataset[\"test\"][\"document\"][10]\n",
    "\n",
    "    \n",
    "tokenised_doc = trained_tokeniser(doc,truncation=True).input_ids # tokenise document\n",
    "decoded_example = tokeniser.decode(tokenised_doc, skip_special_tokens=True) # convert tokenised document back to english\n",
    "\n",
    "\n",
    "print(f'Example document length: {len(doc)}')\n",
    "print(f'Tokenised document length {len(decoded_example)}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-14T19:59:52.238114Z",
     "iopub.execute_input": "2024-01-14T19:59:52.238502Z",
     "iopub.status.idle": "2024-01-14T19:59:52.544505Z",
     "shell.execute_reply.started": "2024-01-14T19:59:52.238465Z",
     "shell.execute_reply": "2024-01-14T19:59:52.543524Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": "Example document length: 33546\nTokenised document length 33545\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, I want to look at the average number of text lost in the test set during the tokenisation process due to truncation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "trained_tokeniser = LEDTokenizer.from_pretrained(\"new_terms_model_4_epoch\")\n",
    "\n",
    "test_set = dataset[\"test\"][\"document\"]\n",
    "\n",
    "untokenised_lengths_arr = []\n",
    "tokenised_lengths_arr = []\n",
    "\n",
    "for doc in test_set:\n",
    "    untokenised_length = len(doc)\n",
    "    untokenised_lengths_arr.append(untokenised_length)\n",
    "    \n",
    "    tokenised_doc = trained_tokeniser(doc,truncation=True).input_ids\n",
    "    decoded_example = tokeniser.decode(tokenised_doc, skip_special_tokens=True)\n",
    "    tokenised_lengths_arr.append(len(decoded_example))\n",
    "    \n",
    "\n",
    "average_length_untokenised = sum(untokenised_lengths_arr) / len(untokenised_lengths_arr)\n",
    "average_length_tokenised = sum(tokenised_lengths_arr) / len(tokenised_lengths_arr)\n",
    "\n",
    "difference = abs(average_length_untokenised - average_length_tokenised)\n",
    "\n",
    "print(f'The untokenised set had an average length of {round(average_length_untokenised)} characters ',\n",
    "      f'compared to the average length of the tokenised documents of {round(average_length_tokenised)} characters ',\n",
    "      f'for a difference of {round(difference)} characters between the two averages')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-14T19:59:52.545731Z",
     "iopub.execute_input": "2024-01-14T19:59:52.546042Z",
     "iopub.status.idle": "2024-01-14T19:59:59.974293Z",
     "shell.execute_reply.started": "2024-01-14T19:59:52.546016Z",
     "shell.execute_reply": "2024-01-14T19:59:59.970995Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": "The untokenised set had an average length of 29213 characters  compared to the average length of the tokenised documents of 28561 characters  for a difference of 653 characters between the two averages\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Set examples\n",
    "\n",
    "To evaluate this model, I will use the hugging Face pipeline to initialise the newly trained model.\n",
    "\n",
    "Recall a test set was set aside and was not used in training or validation, this can be used to see how the model produces summaries on input data is has not yet seen.\n",
    "\n",
    "To start, I want to see how the model summarises some of the examples in the test set, and manually observe the output."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Initialise model from filesystem.\n",
    "trained_model = LEDForConditionalGeneration.from_pretrained(\"/kaggle/working/new_terms_model_4_epoch\")\n",
    "\n",
    "# Use HuggingFace pipeline to pass in the model, tokeniser and tell the pipeline it's a summarisation task.\n",
    "summarizer = pipeline(\"summarization\", model=trained_model,tokenizer=trained_tokeniser)\n",
    "\n",
    "# Pick out and tokenise an example from the test set\n",
    "example = dataset[\"test\"][\"document\"][0]\n",
    "tokenised_example = trained_tokeniser(example,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "# See if any of the input text was truncated during tokenisation\n",
    "print(f'length of example: {len(example)}')\n",
    "tokenised_example_no_tensor = trained_tokeniser(example,truncation=True).input_ids\n",
    "decoded_example = tokeniser.decode(tokenised_example_no_tensor, skip_special_tokens=True)\n",
    "print(f'Length of tokenised example: {len(decoded_example)}')\n",
    "\n",
    "# Generate a summary\n",
    "outputs = trained_model.generate(tokenised_example)\n",
    "\n",
    "# Observe output - skip special tokens such as <s> \n",
    "result = trained_tokeniser.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "ground_truth_summary = dataset[\"test\"][\"summary\"][0]\n",
    "\n",
    "print(f'Ground truth summary: {ground_truth_summary}')\n",
    "print(\" \")\n",
    "print(f'Produced model summary: {result}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-14T19:59:59.976016Z",
     "iopub.execute_input": "2024-01-14T19:59:59.976668Z",
     "iopub.status.idle": "2024-01-14T20:01:29.050148Z",
     "shell.execute_reply.started": "2024-01-14T19:59:59.976625Z",
     "shell.execute_reply": "2024-01-14T20:01:29.049081Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": "length of example: 18010\nLength of tokenised example: 18000\nGround truth summary: You authorise the service to charge a credit card supplied on re-occurring basis. The service does not guarantee accuracy or reliability of the information provided. The service provider makes no warranty regarding uninterrupted, timely, secure or error-free service. The service is provided 'as is' and to be used at the users' sole risk. Invalidity of any portion of the Terms of Service does not entail invalidity of its remainder. The service can delete specific content without reason and may do it without prior notice. You agree to defend, indemnify, and hold the service harmless in case of a claim related to your use of the service. Failure to enforce any provision of the Terms of Service does not constitute a waiver of such provision\n \nProduced model summary: The service is provided 'as is' and to be used at the users' sole risk. The service provider makes no warranty regarding uninterrupted, timely, secure or error-free service. Failure to enforce any provision of the Terms of Service does not constitute a waiver of such provision. This service assumes no liability for any losses or damages resulting from any matter relating to the service. You are responsible for maintaining the security of your account and for the activities on your account. Your account can be deleted without prior notice and without a reason.  Terms may be changed any time at their discretion, without notice to you. Instead of asking directly, this Service will assume your consent merely from your usage.. There is a date of the last update of the agreements. Users agree not to use the service for illegal purposes. They may stop providing the service at any time. User accounts can be terminated after having been in breach of the terms of service repeatedly\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Pick out and tokenise an example from the test set\n",
    "example = dataset[\"test\"][\"document\"][10]\n",
    "tokenised_example = trained_tokeniser(example,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "# See if any of the input text was truncated during tokenisation\n",
    "print(f'length of example: {len(example)}')\n",
    "tokenised_example_no_tensor = trained_tokeniser(example,truncation=True).input_ids\n",
    "decoded_example = tokeniser.decode(tokenised_example_no_tensor, skip_special_tokens=True)\n",
    "print(f'Length of tokenised example: {len(decoded_example)}')\n",
    "\n",
    "# Generate a summary\n",
    "outputs = trained_model.generate(tokenised_example)\n",
    "\n",
    "# Observe output - skip special tokens such as <s> \n",
    "result = trained_tokeniser.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "ground_truth_summary = dataset[\"test\"][\"summary\"][10]\n",
    "\n",
    "print(f'Ground truth summary: {ground_truth_summary}')\n",
    "print(\" \")\n",
    "print(f'Produced model summary: {result}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-14T20:01:29.051828Z",
     "iopub.execute_input": "2024-01-14T20:01:29.052268Z",
     "iopub.status.idle": "2024-01-14T20:03:41.316601Z",
     "shell.execute_reply.started": "2024-01-14T20:01:29.052231Z",
     "shell.execute_reply": "2024-01-14T20:03:41.315619Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": "length of example: 33546\nLength of tokenised example: 33545\nGround truth summary: This service may collect, use, and share location data. You agree not to submit libelous, harassing or threatening content. There is a date of the last update of the agreements. You are responsible for maintaining the security of your account and for the activities on your account. You authorise the service to charge a credit card supplied on re-occurring basis. You must report to the service any unauthorized use of your account or any breach of security. Copyright license limited for the purposes of that same service but transferable and sublicenseable. The service provider makes no warranty regarding uninterrupted, timely, secure or error-free service. If you offer suggestions to the service, they may use that without your approval or compensation, but they do not become the owner. You have a reduced time period to take legal action against the service. This service reserves the right to disclose your personal information without notifying you. The court of law governing the terms is in the state of New York, USA. The service is provided 'as is' and to be used at your sole risk. Any liability on behalf of the service is only limited to the fees you paid as a user or 500$. Your content can be deleted if you violate the terms\n \nProduced model summary: The service is provided 'as is' and to be used at the users' sole risk. The service provider makes no warranty regarding uninterrupted, timely, secure or error-free service. You waive your right to a class action.. This service assumes no liability for any losses or damages resulting from any matter relating to the service.  Terms may be changed any time at their discretion, without notice to you. Instead of asking directly, this Service will assume your consent merely from your usage.. Failure to enforce any provision of the Terms of Service does not constitute a waiver of such provision. Spidering, crawling, or accessing the site through any automated means is not allowed. Your account can be deleted without prior notice and without a reason. There is a date of the last update of the agreements (Terms: November 2021)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating complete test set\n",
    "\n",
    "Recall the test set put to one side earlier has not been seen at all by the model.\n",
    "\n",
    "I will now show these to the model and calculate the ROUGE score on the produced summaries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "summaries = []\n",
    "truths = []\n",
    "count = 1\n",
    "\n",
    "for item in dataset[\"test\"]:\n",
    "    print(f'Processing document {count} of {len(dataset[\"test\"])}')\n",
    "    document = item[\"document\"]\n",
    "    ground_truth_summary = item[\"summary\"]\n",
    "    \n",
    "    tokenised_document = trained_tokeniser(document,return_tensors=\"pt\",truncation=True).input_ids\n",
    "    generated_summary = trained_model.generate(tokenised_document)\n",
    "    decoded_summary = trained_tokeniser.decode(generated_summary[0], skip_special_tokens=True)\n",
    "    \n",
    "    summaries.append(decoded_summary)\n",
    "    truths.append(ground_truth_summary)\n",
    "    count += 1\n",
    "    \n",
    "rouge_score = rouge.compute(predictions=summaries,references=truths)\n",
    "print(rouge_score)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-14T20:03:41.318007Z",
     "iopub.execute_input": "2024-01-14T20:03:41.318384Z",
     "iopub.status.idle": "2024-01-14T21:49:51.986328Z",
     "shell.execute_reply.started": "2024-01-14T20:03:41.318348Z",
     "shell.execute_reply": "2024-01-14T21:49:51.985193Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": "Processing document 1 of 53\nProcessing document 2 of 53\nProcessing document 3 of 53\nProcessing document 4 of 53\nProcessing document 5 of 53\nProcessing document 6 of 53\nProcessing document 7 of 53\nProcessing document 8 of 53\nProcessing document 9 of 53\nProcessing document 10 of 53\nProcessing document 11 of 53\nProcessing document 12 of 53\nProcessing document 13 of 53\nProcessing document 14 of 53\nProcessing document 15 of 53\nProcessing document 16 of 53\nProcessing document 17 of 53\nProcessing document 18 of 53\nProcessing document 19 of 53\nProcessing document 20 of 53\nProcessing document 21 of 53\nProcessing document 22 of 53\nProcessing document 23 of 53\nProcessing document 24 of 53\nProcessing document 25 of 53\nProcessing document 26 of 53\nProcessing document 27 of 53\nProcessing document 28 of 53\nProcessing document 29 of 53\nProcessing document 30 of 53\nProcessing document 31 of 53\nProcessing document 32 of 53\nProcessing document 33 of 53\nProcessing document 34 of 53\nProcessing document 35 of 53\nProcessing document 36 of 53\nProcessing document 37 of 53\nProcessing document 38 of 53\nProcessing document 39 of 53\nProcessing document 40 of 53\nProcessing document 41 of 53\nProcessing document 42 of 53\nProcessing document 43 of 53\nProcessing document 44 of 53\nProcessing document 45 of 53\nProcessing document 46 of 53\nProcessing document 47 of 53\nProcessing document 48 of 53\nProcessing document 49 of 53\nProcessing document 50 of 53\nProcessing document 51 of 53\nProcessing document 52 of 53\nProcessing document 53 of 53\n{'rouge1': 0.49979264704210746, 'rouge2': 0.30839780693301255, 'rougeL': 0.2777833075308299, 'rougeLsum': 0.2784829531804306}\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
