{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install required libraries and imports."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (4.66.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (0.20.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from transformers) (4.66.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install -U transformers\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "# Allocate maximum CUDA memory reserve in an attempt to prevent CUDA out of memory errors\n",
    "# Reserve is simply the reserved memory, not the in-use memory.\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:21.193654300Z",
     "start_time": "2024-01-13T01:36:14.536744900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tell PyTorch to use GPU wherever possible"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:14:28.292515800Z",
     "start_time": "2024-01-13T02:14:28.282516500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, the dataset and checkpoint needs to be initialised."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset location\n",
    "Dataset location depends on where notebook is running, for ease I set it up to just uncomment line below depending on location the notebook is running as I run the notebook in a lot of locations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#dataset_location = \"/kaggle/input/new-datasets/Privacy_Policy_dataset.jsonl\" # Kaggle\n",
    "\n",
    "dataset_location = \"Privacy_Policy_dataset.jsonl\" # Local / Google Colab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:21.238664900Z",
     "start_time": "2024-01-13T01:36:21.195654200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialise dataset\n",
    "To initialise the dataset I use the \"dataset\" library from python.\n",
    "\n",
    "I split the dataset into three sets:\n",
    "- Training set - The data shown to the model during training\n",
    "- Validation - The data shown to the model to calculate loss on backward pass\n",
    "- Test - Reserved strictly for after the model is trained, used to evaluate the model on a completely unseen set\n",
    "\n",
    "However, the \"datasets\" library doesn't offer the possibility to split into three sets so I use a workaround sourced from: [This stackoverflow post](https://stackoverflow.com/questions/76001128/splitting-dataset-into-train-test-and-validation-using-huggingface-datasets-fun)\n",
    "\n",
    "It works by first splitting the data set into a train set (80%) and a validation set (20%).\n",
    "\n",
    "It then splits this validation set into a train set and validation set of 50% each, resulting in two sets of 10% each.\n",
    "\n",
    "A final dataset is then built using these split datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['summary', 'document'],\n",
      "        num_rows: 453\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['summary', 'document'],\n",
      "        num_rows: 57\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['summary', 'document'],\n",
      "        num_rows: 57\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=dataset_location,split='train')\n",
    "\n",
    "dataset = dataset.shuffle(seed=2424)\n",
    "\n",
    "test_valid_split_dataset = dataset.train_test_split(test_size=0.2, shuffle=False)\n",
    "\n",
    "test_split = test_valid_split_dataset['test'].train_test_split(test_size=0.5, shuffle = False)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': test_valid_split_dataset['train'],\n",
    "    'test': test_split['test'],\n",
    "    'valid': test_split['train']})\n",
    "\n",
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:21.948222400Z",
     "start_time": "2024-01-13T01:36:21.212681500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Review dataset\n",
    "Next I want to see the properties of the dataset, to understand what i'm working with.\n",
    "\n",
    "For training a summarisation model knowing the length of the collected documents is crucial.\n",
    "\n",
    "The largest summarisation base model is only capable of processing 16384 tokens - higher token limits is a limitation in NLP as a whole.\n",
    "\n",
    "Roughly we can equate [One token to be equal to about 4 English characters)[https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them].\n",
    "\n",
    "This gives roughly 65,536 characters which the model will be able to parse at once, and anything which exceeds this number needs to be truncated down to the 16384 token limit.\n",
    "\n",
    "This unfortunately means on some documents, some detail will be missing.\n",
    "\n",
    "However, as visible below, the average document is just 24,724 considerably under the maximum token limit, thus, for most items in the dataset this isn't a problem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the first privacy policy in the train dataset is 5342 characters\n",
      "Length of the longest privacy policy in the train dataset is 261619 characters\n",
      "Length of the shortest privacy policy in the train dataset is 826 characters\n",
      "The average character count in the dataset is: 24724\n"
     ]
    }
   ],
   "source": [
    "length_of_first_item = len(dataset['train'][0]['document'])\n",
    "print(f'The length of the first privacy policy in the train dataset is {length_of_first_item} characters')\n",
    "\n",
    "length_of_longest_document = len(max(dataset['train'], key=lambda x: len(x['document']))['document'])\n",
    "print(f'Length of the longest privacy policy in the train dataset is {length_of_longest_document} characters')\n",
    "\n",
    "length_of_shortest_document = len(min(dataset['train'], key=lambda x: len(x['document']))['document'])\n",
    "print(f'Length of the shortest privacy policy in the train dataset is {length_of_shortest_document} characters')\n",
    "\n",
    "total_char_count = sum(map(len, dataset['train']['document']))\n",
    "avg_char_count = round(total_char_count / len(dataset['train']['document']))\n",
    "\n",
    "print(f'The average character count in the dataset is: {avg_char_count}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.059222300Z",
     "start_time": "2024-01-13T01:36:21.951221600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Base model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I will be utilising transfer learning to train a model.\n",
    "\n",
    "This takes the base of a model trained on some other task but in a similar domain (e.g. summarising books), removes the head of the model which is more specialised (e.g. contains information specific to books), while retaining useful information about the English language. The model is then trained on a new specific task, in my case, summarising terms and conditions or privacy policies, utilising its pre-existing knowledge of the English language.\n",
    "\n",
    "This significantly reduces training time and resources required for training such that I can stay within the final year project deadlines.\n",
    "\n",
    "The model I will be using as a base is the [led-large-book-summary](https://huggingface.co/pszemraj/led-large-book-summary). This model utilises the Longformer Encoder-Decoder (LED) model as it's base, and was trained further to summarise long-form text such as novels, plays and stories from the [BookSum dataset](https://arxiv.org/abs/2105.08209)\n",
    "\n",
    " Below I initialise the tokeniser for this model through the [Hugging Face](https://huggingface.co/models) library, which offer a variety of base models for transfer-learning.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_name = \"pszemraj/led-large-book-summary\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(base_model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.618471300Z",
     "start_time": "2024-01-13T01:36:22.061221600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformer models *only* take numerical inputs, thus, a tokeniser is responsible for transforming the input text into its numerical representation.\n",
    "\n",
    "It does this by splitting text into \"tokens\" which are small groups of characters.\n",
    "\n",
    "Tokenisers also use special characters, often indicating the start and the end of sequences and words.\n",
    "\n",
    "Below I display an example by tokenising a simple string using the tokeniser for the \"led-large-book-summary\" model\n",
    "\n",
    "The `input_ids` represents the tokenised input, `attention_mask` is tells the model to ignore tokens if the equivalent index in the attention mask array is zero.\n",
    "\n",
    "Note when converting the string back to English, we can see`<s>` being used to indicate the start of a sequence, and each word beginning with `Ġ`.\n",
    " - This changes depending on the base model, but the Hugging Face library picks out the right tokeniser for the base model.\n",
    "\n",
    "Also, words can be split into two tokens if it is deemed useful, below `tokeniser` is split into two tokens `token` and `iser`, as the tokens `iser` and `token` could later be re-used with other words, saving tokens."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 713, 16, 10, 1296, 6755, 7, 1296, 66, 5, 19233, 5999, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": "['<s>',\n 'This',\n 'Ġis',\n 'Ġa',\n 'Ġtest',\n 'Ġstring',\n 'Ġto',\n 'Ġtest',\n 'Ġout',\n 'Ġthe',\n 'Ġtoken',\n 'iser',\n '</s>']"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = tokeniser(\"This is a test string to test out the tokeniser\")\n",
    "print(test_string)\n",
    "tokeniser.convert_ids_to_tokens(test_string.input_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.629472300Z",
     "start_time": "2024-01-13T01:36:22.614472800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, a function is needed which will tokenise the text.\n",
    "\n",
    "Here, the maximum length can be defined as 16384 tokens and the tokeniser will be responsible for ensuring any text exceeding this is truncated.\n",
    "\n",
    "In the dataset, the \"document\" column contains the input document (privacy policy or Terms of Service)\n",
    "\n",
    "The \"summary\" column contains the ground truth summary for the matching document.\n",
    " - This doesn't need to be truncated, as they are all <500 tokens\n",
    "\n",
    "Furthermore, this function assigns the tokenised ground truth summaries `input_id`'s to the \"labels\" property of the tokenised documents.\n",
    " - This is the format required for training a summarisaiton model in pytorch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def tokenise_truncate_dataset(input):\n",
    "    truncated_input = tokeniser(\n",
    "        input[\"document\"],\n",
    "        max_length = 16384,\n",
    "        truncation = True\n",
    "    )\n",
    "    labels = tokeniser( # don't truncate labels\n",
    "        input[\"summary\"],\n",
    "        truncation = False,\n",
    "    )\n",
    "    truncated_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return truncated_input\n",
    "# By passing `batched = true` into the map function, more than one item is applied to the function at a time.\n",
    "tokenised_dataset = dataset.map(tokenise_truncate_dataset, batched = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.723472600Z",
     "start_time": "2024-01-13T01:36:22.631472300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The properties of the dataset can now be viewed, as expected, there is a train, test and validation dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 453\n    })\n    test: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 57\n    })\n    valid: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 57\n    })\n})"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.740471900Z",
     "start_time": "2024-01-13T01:36:22.725471800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, the \"summary\" and \"document\" dataset are no longer needed, as we have their tokenised equivalents - \"input_ids\" and \"labels\".\n",
    "\n",
    "Thus, these can be removed.\n",
    "\n",
    "Furthermore, the dataset needs to be set to return pytorch tensors, in order to be able to be trained in pytorch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 453\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 57\n    })\n    valid: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 57\n    })\n})"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_dataset = tokenised_dataset.remove_columns([\"summary\",\"document\"])\n",
    "tokenised_dataset.set_format(\"torch\")\n",
    "tokenised_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.783937100Z",
     "start_time": "2024-01-13T01:36:22.741474200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, a data collator needs to be defined.\n",
    "\n",
    "(Below Information sourced from: https://huggingface.co/docs/transformers/main_classes/data_collator)\n",
    "\n",
    "This is responsible for constructing batches and applying pre-processing such as padding to ensure all inputs are of the same size.\n",
    "\n",
    "The Hugging Face library provides a function for sourcing a data collator with padding."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:42:02.436350100Z",
     "start_time": "2024-01-13T01:42:01.202269900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, Pytorch dataloaders need to be defined, in order to load the data into the model.\n",
    "\n",
    "A *very* small batch size is used, of just 1, as training this model uses *a lot* of GPU RAM, and anything higher than this causes Kaggle / Colab to crash."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenised_dataset[\"train\"], batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenised_dataset[\"valid\"], batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenised_dataset[\"test\"], batch_size=1, collate_fn=data_collator\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:42:53.432954300Z",
     "start_time": "2024-01-13T01:42:53.420958Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "First, the base model needs to be defined.\n",
    "\n",
    "Next, an optimiser needs to be defined.\n",
    "\n",
    "The standard optimiser to use is `adamW`, but again, due to VRAM limitations, a less memory-intensive optimiser will be used.\n",
    "\n",
    "The learning rate and base model parameters will be passed to the optimiser.\n",
    "\n",
    "relative_step will be set to false to define a custom learning rate, which I used in previous models and found to be good."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "from transformers.optimization import Adafactor\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "optimiser = Adafactor(base_model.parameters(),lr=2e-5, relative_step=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:01:43.238089600Z",
     "start_time": "2024-01-13T02:01:40.995123800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(This stackoverflow post helped with defining training steps https://stackoverflow.com/questions/60120043/optimizer-and-scheduler-for-bert-fine-tuning)\n",
    "\n",
    "Next, the *learning rate scheduler* needs to be defined, which is responsible for reducing this learning rate as training continues.\n",
    "\n",
    "The simplest implementation is to just handle this linearly by multiplying the number of epochs by the number of training items, to calculate the number of \"training steps\", then define a learning rate scheduler to handle this linearly.\n",
    "\n",
    "The number of warm up steps is typically set to zero."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from transformers.optimization import get_scheduler\n",
    "\n",
    "epochs = 4\n",
    "training_steps = epochs * len(train_dataloader)\n",
    "learning_rate_scheduler = get_scheduler(\"linear\",optimizer=optimiser,num_warmup_steps=0,num_training_steps=training_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:12:33.630546100Z",
     "start_time": "2024-01-13T02:12:33.609547200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, the base model needs to be assigned to run the GPU\n",
    "\n",
    "If the output is 'cuda' then a GPU is assigned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:14:20.537563700Z",
     "start_time": "2024-01-13T02:14:20.522563800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default PyTorch doesn't offer any visualisation of model training progress, so the library `tqdm` can visualise this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "progress_bar = tqdm(range(training_steps))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, ROUGE needs to be defined for the evaluation loop."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (0.4.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from evaluate) (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from evaluate) (1.25.2)\n",
      "Requirement already satisfied: dill in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from evaluate) (4.66.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from evaluate) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from evaluate) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from evaluate) (0.20.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (13.0.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: rouge_score in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from rouge_score) (2.0.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from rouge_score) (1.25.2)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from nltk->rouge_score) (8.1.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from nltk->rouge_score) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from nltk->rouge_score) (4.66.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\logan\\anaconda3\\envs\\gpu_env\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:18:36.724807300Z",
     "start_time": "2024-01-13T02:18:26.022709800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, the model training loop needs to be defined:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = base_model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "        learning_rate_scheduler.step()\n",
    "        optimiser.zero_grad()\n",
    "        progress_bar.update(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, the evaluation loop to be used during model training needs to be defined.\n",
    "\n",
    "This will run against the evaluation set (eval_dataloader) defined earlier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = base_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    rouge.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "rouge.compute()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
