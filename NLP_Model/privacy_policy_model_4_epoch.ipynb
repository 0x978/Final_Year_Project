{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7293442,
     "sourceType": "datasetVersion",
     "datasetId": 4230174
    },
    {
     "sourceId": 7293534,
     "sourceType": "datasetVersion",
     "datasetId": 4230224
    },
    {
     "sourceId": 7300111,
     "sourceType": "datasetVersion",
     "datasetId": 4234911
    },
    {
     "sourceId": 7320296,
     "sourceType": "datasetVersion",
     "datasetId": 4248138
    },
    {
     "sourceId": 7326507,
     "sourceType": "datasetVersion",
     "datasetId": 4252461
    },
    {
     "sourceId": 7333022,
     "sourceType": "datasetVersion",
     "datasetId": 4256963
    },
    {
     "sourceId": 7392147,
     "sourceType": "datasetVersion",
     "datasetId": 4297367
    }
   ],
   "dockerImageVersionId": 30626,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Install required libraries and imports.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -U transformers\n!pip3 install torch --index-url https://download.pytorch.org/whl/cu118\n!pip install -U datasets\nfrom datasets import load_dataset, DatasetDict\nimport os\n\n# Allocate maximum CUDA memory reserve in an attempt to prevent CUDA out of memory errors\n# Reserve is simply the reserved memory, not the in-use memory.\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\"",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:21.193654300Z",
     "start_time": "2024-01-13T01:36:14.536744900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:32:54.691074Z",
     "iopub.execute_input": "2024-01-13T21:32:54.691954Z",
     "iopub.status.idle": "2024-01-13T21:33:48.451395Z",
     "shell.execute_reply.started": "2024-01-13T21:32:54.691871Z",
     "shell.execute_reply": "2024-01-13T21:33:48.450182Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\nCollecting transformers\n  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/20/0a/739426a81f7635b422fbe6cb8d1d99d1235579a6ac8024c13d743efa6847/transformers-4.36.2-py3-none-any.whl.metadata\n  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m126.8/126.8 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.2/8.2 MB\u001B[0m \u001B[31m54.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.36.0\n    Uninstalling transformers-4.36.0:\n      Successfully uninstalled transformers-4.36.0\nSuccessfully installed transformers-4.36.2\nLooking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting datasets\n  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/ec/93/454ada0d1b289a0f4a86ac88dbdeab54921becabac45da3da787d136628f/datasets-2.16.1-py3-none-any.whl.metadata\n  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.12.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nCollecting pyarrow-hotfix (from datasets)\n  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nCollecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets)\n  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m507.1/507.1 kB\u001B[0m \u001B[31m10.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.4/166.4 kB\u001B[0m \u001B[31m16.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: pyarrow-hotfix, fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.12.2\n    Uninstalling fsspec-2023.12.2:\n      Successfully uninstalled fsspec-2023.12.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed datasets-2.16.1 fsspec-2023.10.0 pyarrow-hotfix-0.6\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Tell PyTorch to use GPU wherever possible",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:14:28.292515800Z",
     "start_time": "2024-01-13T02:14:28.282516500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:33:48.453164Z",
     "iopub.execute_input": "2024-01-13T21:33:48.453653Z",
     "iopub.status.idle": "2024-01-13T21:33:51.637434Z",
     "shell.execute_reply.started": "2024-01-13T21:33:48.453624Z",
     "shell.execute_reply": "2024-01-13T21:33:51.636509Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Data processing",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "First, the dataset and checkpoint needs to be initialised.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Dataset location\nDataset location depends on where notebook is running, for ease I set it up to just uncomment line below depending on location the notebook is running as I run the notebook in a lot of locations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "dataset_location = \"/kaggle/input/new-datasets/Privacy_Policy_dataset.jsonl\" # Kaggle\n\n#dataset_location = \"Privacy_Policy_dataset.jsonl\" # Local / Google Colab",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:21.238664900Z",
     "start_time": "2024-01-13T01:36:21.195654200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:33:51.638672Z",
     "iopub.execute_input": "2024-01-13T21:33:51.639127Z",
     "iopub.status.idle": "2024-01-13T21:33:51.643391Z",
     "shell.execute_reply.started": "2024-01-13T21:33:51.639100Z",
     "shell.execute_reply": "2024-01-13T21:33:51.642467Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Initialise dataset\nTo initialise the dataset I use the \"dataset\" library from python.\n\nI split the dataset into three sets:\n- Training set - The data shown to the model during training\n- Validation - The data shown to the model to calculate loss on backward pass\n- Test - Reserved strictly for after the model is trained, used to evaluate the model on a completely unseen set\n\nHowever, the \"datasets\" library doesn't offer the possibility to split into three sets so I use a workaround sourced from: [This stackoverflow post](https://stackoverflow.com/questions/76001128/splitting-dataset-into-train-test-and-validation-using-huggingface-datasets-fun)\n\nIt works by first splitting the data set into a train set (80%) and a validation set (20%).\n\nIt then splits this validation set into a train set and validation set of 50% each, resulting in two sets of 10% each.\n\nA final dataset consisting of a train, test and validation set is then built using these split datasets",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "dataset = load_dataset(\"json\", data_files=dataset_location,split='train')\n\ndataset = dataset.shuffle(seed=2424)\n\ntest_valid_split_dataset = dataset.train_test_split(test_size=0.2, shuffle=False)\n\ntest_split = test_valid_split_dataset['test'].train_test_split(test_size=0.5, shuffle = False)\n\ndataset = DatasetDict({\n    'train': test_valid_split_dataset['train'],\n    'test': test_split['test'],\n    'valid': test_split['train']})\n\nprint(dataset)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:21.948222400Z",
     "start_time": "2024-01-13T01:36:21.212681500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:33:51.645611Z",
     "iopub.execute_input": "2024-01-13T21:33:51.645873Z",
     "iopub.status.idle": "2024-01-13T21:33:52.130894Z",
     "shell.execute_reply.started": "2024-01-13T21:33:51.645850Z",
     "shell.execute_reply": "2024-01-13T21:33:52.129791Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae071a1461004b478478ffb4a7be78f8"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "DatasetDict({\n    train: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 453\n    })\n    test: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 57\n    })\n    valid: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 57\n    })\n})\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Reviewing the dataset\nNext I want to see the properties of the dataset, to understand what i'm working with.\n\nFor training a summarisation model knowing the length of the collected documents is crucial.\n\nThe largest summarisation base model is only capable of processing 16384 tokens - higher token limits is a limitation in NLP as a whole.\n\nRoughly, [One token is equal to about four English characters](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)\n\nThis gives roughly 65,536 characters which the model will be able to parse and create a summary for, and anything which exceeds this number needs to be truncated down to the 16384 token limit.\n\nThis unfortunately means on some documents, some detail will be missing.\n\nHowever, as visible below, the average document in the train set is just 24,724 characters, considerably under the maximum token limit, thus, for most items in the dataset this isn't a problem.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "length_of_first_item = len(dataset['train'][0]['document'])\nprint(f'The length of the first privacy policy in the train dataset is {length_of_first_item} characters')\n\nlength_of_longest_document = len(max(dataset['train'], key=lambda x: len(x['document']))['document'])\nprint(f'Length of the longest privacy policy in the train dataset is {length_of_longest_document} characters')\n\nlength_of_shortest_document = len(min(dataset['train'], key=lambda x: len(x['document']))['document'])\nprint(f'Length of the shortest privacy policy in the train dataset is {length_of_shortest_document} characters')\n\ntotal_char_count = sum(map(len, dataset['train']['document']))\navg_char_count = round(total_char_count / len(dataset['train']['document']))\n\nprint(f'The average character count in the dataset is: {avg_char_count}')",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.059222300Z",
     "start_time": "2024-01-13T01:36:21.951221600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:33:52.132170Z",
     "iopub.execute_input": "2024-01-13T21:33:52.132501Z",
     "iopub.status.idle": "2024-01-13T21:33:52.282233Z",
     "shell.execute_reply.started": "2024-01-13T21:33:52.132472Z",
     "shell.execute_reply": "2024-01-13T21:33:52.281258Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "The length of the first privacy policy in the train dataset is 5342 characters\nLength of the longest privacy policy in the train dataset is 261619 characters\nLength of the shortest privacy policy in the train dataset is 826 characters\nThe average character count in the dataset is: 24724\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Base model",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "I will be utilising transfer learning to train a model.\n\nThis takes the base of a model trained on some other task but in a similar domain (e.g. summarising books), removes the head of the model which is more specialised (e.g. contains information specific to books), while retaining useful information about the English language. The model is then trained on a new specific task, in my case, summarising terms and conditions or privacy policies, utilising its pre-existing knowledge of the English language.\n\nThis significantly reduces training time and resources required for training such that I can stay within the final year project deadlines.\n\nThe model I will be using as a base is the [led-large-book-summary](https://huggingface.co/pszemraj/led-large-book-summary). This model utilises the Longformer Encoder-Decoder (LED) model as it's base, and was trained further to summarise long-form text such as novels, plays and stories from the [BookSum dataset](https://arxiv.org/abs/2105.08209)\n\n Below I initialise the tokeniser for this model through the [Hugging Face](https://huggingface.co/models) library, which offer a variety of base models for transfer-learning. \n  - This base model is ~1.5 GB \n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer\n\nbase_model_name = \"pszemraj/led-large-book-summary\"\ntokeniser = AutoTokenizer.from_pretrained(base_model_name)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.618471300Z",
     "start_time": "2024-01-13T01:36:22.061221600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:33:52.283651Z",
     "iopub.execute_input": "2024-01-13T21:33:52.284317Z",
     "iopub.status.idle": "2024-01-13T21:33:54.784990Z",
     "shell.execute_reply.started": "2024-01-13T21:33:52.284279Z",
     "shell.execute_reply": "2024-01-13T21:33:54.783956Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "addac032c1c94e579eda098ce27be190"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "716cf0b767914ac3ad3dfe179877d474"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7d4a165da76440ab520db9551bc1f49"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9ef7e44e74c4ae2b1831d3554828b6d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57fa317468b84fffa69cc222dc458361"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Transformer models *only* take numerical inputs, thus, a tokeniser is responsible for transforming the input text into its numerical representation.\n\nIt does this by splitting text into \"tokens\" which are small groups of characters.\n\nTokenisers also use special characters, often indicating the start and the end of sequences and words.\n\nBelow I display an example by tokenising a simple string using the tokeniser for the \"led-large-book-summary\" model\n\nThe `input_ids` represents the tokenised input, `attention_mask` is tells the model to ignore tokens if the equivalent index in the attention mask array is zero.\n\nNote when converting the string back to English, we can see`<s>` being used to indicate the start of a sequence, and each word beginning with `Ġ`.\n - This changes depending on the base model, but the Hugging Face library picks out the right tokeniser for the base model.\n\nAlso, words can be split into two tokens if it is deemed useful, below `tokeniser` is split into two tokens `token` and `iser`, as the tokens `iser` and `token` could later be re-used with other words, saving tokens.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "test_string = tokeniser(\"This is a test string to test out the tokeniser\")\nprint(test_string)\ntokeniser.convert_ids_to_tokens(test_string.input_ids)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.629472300Z",
     "start_time": "2024-01-13T01:36:22.614472800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:33:54.786337Z",
     "iopub.execute_input": "2024-01-13T21:33:54.786815Z",
     "iopub.status.idle": "2024-01-13T21:33:54.799060Z",
     "shell.execute_reply.started": "2024-01-13T21:33:54.786787Z",
     "shell.execute_reply": "2024-01-13T21:33:54.798090Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'input_ids': [0, 713, 16, 10, 1296, 6755, 7, 1296, 66, 5, 19233, 5999, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
     "output_type": "stream"
    },
    {
     "execution_count": 7,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['<s>',\n 'This',\n 'Ġis',\n 'Ġa',\n 'Ġtest',\n 'Ġstring',\n 'Ġto',\n 'Ġtest',\n 'Ġout',\n 'Ġthe',\n 'Ġtoken',\n 'iser',\n '</s>']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Next, a function is needed which will tokenise the text.\n\nHere, the maximum length can be defined as 16384 tokens and the tokeniser will be responsible for ensuring any text exceeding this is truncated.\n\nIn the dataset, the \"document\" column contains the input document (privacy policy or Terms of Service)\n\nThe \"summary\" column contains the ground truth summary for the matching document.\n - This doesn't need to be truncated, as they are all <500 tokens\n\nFurthermore, this function assigns the tokenised ground truth summaries `input_id`'s to the \"labels\" property of the tokenised documents.\n - This is the format required for training a summarisaiton model in pytorch.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def tokenise_truncate_dataset(input):\n    truncated_input = tokeniser(\n        input[\"document\"],\n        max_length = 16384,\n        truncation = True\n    )\n    labels = tokeniser( # don't truncate labels\n        input[\"summary\"],\n        truncation = False,\n    )\n    truncated_input[\"labels\"] = labels[\"input_ids\"]\n    return truncated_input\n# By passing `batched = true` into the map function, more than one item is applied to the function at a time.\ntokenised_dataset = dataset.map(tokenise_truncate_dataset, batched = True)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.723472600Z",
     "start_time": "2024-01-13T01:36:22.631472300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:33:54.800571Z",
     "iopub.execute_input": "2024-01-13T21:33:54.801177Z",
     "iopub.status.idle": "2024-01-13T21:34:00.723535Z",
     "shell.execute_reply.started": "2024-01-13T21:33:54.801141Z",
     "shell.execute_reply": "2024-01-13T21:34:00.722542Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/453 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10e691f14ec947ebbece45a9b4f5ea9a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/57 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ffbc76285164f4093674a9369b62249"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/57 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4e1d0f33e104d71a0837bff10266b8f"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "The properties of the dataset can now be viewed, as expected, there is a train, test and validation dataset.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "tokenised_dataset",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.740471900Z",
     "start_time": "2024-01-13T01:36:22.725471800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:34:00.724721Z",
     "iopub.execute_input": "2024-01-13T21:34:00.725025Z",
     "iopub.status.idle": "2024-01-13T21:34:00.731474Z",
     "shell.execute_reply.started": "2024-01-13T21:34:00.724998Z",
     "shell.execute_reply": "2024-01-13T21:34:00.730260Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 453\n    })\n    test: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 57\n    })\n    valid: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 57\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "However, the \"summary\" and \"document\" dataset are no longer needed, as we have their tokenised equivalents - \"input_ids\" and \"labels\".\n\nThus, these can be removed.\n\nFurthermore, the dataset needs to be set to return pytorch tensors, in order to be able to be trained in pytorch.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "tokenised_dataset = tokenised_dataset.remove_columns([\"summary\",\"document\"])\ntokenised_dataset.set_format(\"torch\")\ntokenised_dataset",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.783937100Z",
     "start_time": "2024-01-13T01:36:22.741474200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:34:00.734710Z",
     "iopub.execute_input": "2024-01-13T21:34:00.735011Z",
     "iopub.status.idle": "2024-01-13T21:34:00.764123Z",
     "shell.execute_reply.started": "2024-01-13T21:34:00.734985Z",
     "shell.execute_reply": "2024-01-13T21:34:00.763245Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "execution_count": 10,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 453\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 57\n    })\n    valid: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 57\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Next, a data collator needs to be defined.\n\n(Below Information sourced from: https://huggingface.co/docs/transformers/main_classes/data_collator)\n\nThis is responsible for constructing batches and applying pre-processing such as padding to ensure all inputs are of the same size.\n\nThe Hugging Face library provides a function for sourcing a data collator with padding.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokeniser, model= base_model_name)\n",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:42:02.436350100Z",
     "start_time": "2024-01-13T01:42:01.202269900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:34:00.765250Z",
     "iopub.execute_input": "2024-01-13T21:34:00.765540Z",
     "iopub.status.idle": "2024-01-13T21:34:11.677054Z",
     "shell.execute_reply.started": "2024-01-13T21:34:00.765514Z",
     "shell.execute_reply": "2024-01-13T21:34:11.676044Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Model\n\nFirst, the base model needs to be defined.\n\nNext, an optimiser needs to be defined.\n\nThe standard optimiser to use is `adamW`, but again, due to VRAM limitations, a less memory-intensive optimiser will be used called \"adafactor\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\nfrom transformers.optimization import Adafactor\n\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:01:43.238089600Z",
     "start_time": "2024-01-13T02:01:40.995123800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:34:11.678519Z",
     "iopub.execute_input": "2024-01-13T21:34:11.679365Z",
     "iopub.status.idle": "2024-01-13T21:34:22.383420Z",
     "shell.execute_reply.started": "2024-01-13T21:34:11.679324Z",
     "shell.execute_reply": "2024-01-13T21:34:22.382269Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdb15ff86f9f4db8b375b9d18b931969"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad5d2eeb09384a81a5987bfd5a8137c5"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Next, ROUGE needs to be defined for the evaluation loop, and a function needs to be defined which will compute the rouge scores.\n\nThe rouge score metric is imported from the `rouge_score` python library, which will calculate and return the following metrics\n \n ### ROUGE-1\n ROUGE-1 is the overlap of words between the produced summary and ground truth summary.\n $$\\frac{overlapping \\space words}{total \\space words}$$\n \n ### ROUGE-2\n ROUGE-2 is the overlap of bi-grams (pairs of words)\n \n $$\\frac{overlapping \\space pairs}{total \\space number \\space of \\space bi-grams}$$ \n \n ### ROUGE-L\n Source partially from (https://en.wikipedia.org/wiki/Longest_common_subsequence)\n \n ROUGE-L is based on the idea of a *longest common subsequence* - the longest common sequence of words between two texts.\n \n A subsequence does not necessarily have to be *consecutive*, letters can be skipped to make a subsequence.\n \n Consider strings \"ABCD\" and \"ACBAD\" - the longest common subseqeunces sequences are ABD and ACD.\n \n $$\\frac{Longest \\space common \\space subsequence}{total \\space words}$$ \n \n Such that for the above example, if we consider \"ABCD\" the produced text and \"ACBAD\" the ground truth, ROUGE-L is equal to $ \\frac{3}{5} = 0.6$\n \n ### ROUGE-L-SUM\n sourced from https://dev.to/aws-builders/mastering-rouge-matrix-your-guide-to-large-language-model-evaluation-for-summarization-with-examples-jjg\n \nThis is similar to ROUGE-L, but instead compares this at a *sentence* level, calculating ROUGE-L for each sentence in the produced summary.\n\nThis is a better measurement of accuracy in my use-case, as the produced summaries are split into sentences, where each sentence is a \"summary point\".",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install evaluate\n!pip install rouge_score\n\nimport evaluate\nimport numpy as np\n\nrouge = evaluate.load(\"rouge\")\n\ndef decode_and_find_rouge(result):\n    predictions,labels = result # extract predictions and labels from the passed in result.\n\n    # Decode by converting tokenised inputs back into English representations\n    decoded_predictions = tokeniser.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokeniser.batch_decode(labels, skip_special_tokens = True)\n    \n    # Calculate rouge scores by passing in predictions and ground truths\n    rouge_score = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n    \n    # Calculate the average generated length fo examples.\n    num_of_predictions = [np.count_nonzero(pred != tokeniser.pad_token_id) for pred in predictions]\n    rouge_score[\"gen_len\"] = np.mean(num_of_predictions)\n\n    # Round and return results.\n    return {k: round(v, 4) for k, v in rouge_score.items()}",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:18:36.724807300Z",
     "start_time": "2024-01-13T02:18:26.022709800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:34:22.384815Z",
     "iopub.execute_input": "2024-01-13T21:34:22.385149Z",
     "iopub.status.idle": "2024-01-13T21:35:00.935136Z",
     "shell.execute_reply.started": "2024-01-13T21:34:22.385123Z",
     "shell.execute_reply": "2024-01-13T21:35:00.934078Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Collecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.16.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.12.2)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.24.3)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=13111d44b393f6651ada33ed7d0360c7c21d7d5b5bba946fecf7657c84fbf6ad\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e190217fac65437b98cff7e8011cf5dd"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Next, the model training loop needs to be defined:\n\nThis is a very expensive model on memory and it is essential to avoid exceeding the GPU's VRAM, thus the hyper-parameters are configured as such:\n\n1) The batch size will be set to just 1 to reduce the amount of data stored in memory.\n\n2) 16-bit floating point precision will be used, rather than 32-bit.\n\n3) The optimiser \"adafactor\" is used over the industry standard of \"Adam\". As a result, the model converges slower but also uses less memory.\n\n4) \"Gradient Checkpointing\" is used, instructing the model to forget the majority of forward-pass activations and instead recompute them on demand during the backward pass, saving only the \"most important\" activations. Significantly slower training time, but also saves a lot of VRAM.\n\n5) \"Gradient Accumulation\" is used simulate a \"larger\" effective batch size. Instead of updating the model's parameters after processing eachbatch, the gradients are accumulated over 2 batches before performing a single update.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n\nmodels_arguments = Seq2SeqTrainingArguments(\n    output_dir=\"new_privacy_policy_model_4_epoch\",\n    evaluation_strategy=\"epoch\", # Run evaluation function on each epoch\n    learning_rate=2e-5, # learning rate hyperparameter set to 0.00002\n    per_device_train_batch_size= 1, # split into batches of 1 for training\n    per_device_eval_batch_size=1, # split to batches of 1 for evaluation\n    weight_decay=0.01, # Utilises L2 regularization in an attempt to prevent overfitting\n    save_total_limit=3, # save 3 checkpoints only and delete older checkpoints (Kept using all RAM without)\n    num_train_epochs=4, # train for 4 epochs\n    predict_with_generate=True, # Generate summaries for each input ; essential for summarisation tasks\n    fp16=True, # use 16 bit floating point - reduced memory usage\n    gradient_accumulation_steps=2,\n    gradient_checkpointing=True,\n    optim=\"adafactor\"\n)\n\n# Collect the previously defined trainer parameters, such as the evaluation technique, tokeniser and datasets.\ntrainer = Seq2SeqTrainer(\n    model=base_model,\n    args=models_arguments,\n    train_dataset=tokenised_dataset[\"train\"],\n    eval_dataset=tokenised_dataset[\"valid\"],\n    tokenizer=tokeniser,\n    data_collator=data_collator,\n    compute_metrics=decode_and_find_rouge,\n)\n\nimport os\n\n# Uncomment below line if using Kaggle.\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntrainer.train()\ntrainer.save_model(\"new_privacy_policy_model_4_epoch\")",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T21:35:00.936834Z",
     "iopub.execute_input": "2024-01-13T21:35:00.937672Z",
     "iopub.status.idle": "2024-01-13T23:52:42.588558Z",
     "shell.execute_reply.started": "2024-01-13T21:35:00.937630Z",
     "shell.execute_reply": "2024-01-13T23:52:42.587397Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a LEDTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='904' max='904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [904/904 2:16:58, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>0.638767</td>\n      <td>0.426000</td>\n      <td>0.243800</td>\n      <td>0.235900</td>\n      <td>0.235800</td>\n      <td>293.017500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.782500</td>\n      <td>0.564965</td>\n      <td>0.507300</td>\n      <td>0.315700</td>\n      <td>0.272100</td>\n      <td>0.271900</td>\n      <td>183.438600</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.782500</td>\n      <td>0.560927</td>\n      <td>0.488300</td>\n      <td>0.294600</td>\n      <td>0.270900</td>\n      <td>0.270800</td>\n      <td>145.578900</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Evaluation",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  }
 ]
}
