{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 7293442,
     "sourceType": "datasetVersion",
     "datasetId": 4230174
    },
    {
     "sourceId": 7293534,
     "sourceType": "datasetVersion",
     "datasetId": 4230224
    },
    {
     "sourceId": 7300111,
     "sourceType": "datasetVersion",
     "datasetId": 4234911
    },
    {
     "sourceId": 7320296,
     "sourceType": "datasetVersion",
     "datasetId": 4248138
    },
    {
     "sourceId": 7326507,
     "sourceType": "datasetVersion",
     "datasetId": 4252461
    },
    {
     "sourceId": 7333022,
     "sourceType": "datasetVersion",
     "datasetId": 4256963
    },
    {
     "sourceId": 7363960,
     "sourceType": "datasetVersion",
     "datasetId": 4277873
    }
   ],
   "dockerImageVersionId": 30626,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning led-large-book-summary on a joint dataset\n",
    "\n",
    "This notebook will go through data-processing and training of a the NLP model responsible for summarising terms and conditions and privacy policies, using the \"led-large-book-summary\" as a base model to be fine-tuned.\n",
    "\n",
    "led-large-book-summary: https://huggingface.co/pszemraj/led-large-book-summary\n",
    "\n",
    "This will be a very similar procedure to training previous models, but with a dataset containing both terms and conditons, privacy policies and their respective summaries.\n",
    "\n",
    "**This model needs ~16GB of VRAM for training, running on any lower will give a CUDA out of memory error.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-Processing\n",
    "First, import and install required libraries"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-28T17:51:33.438989Z",
     "iopub.execute_input": "2023-12-28T17:51:33.439335Z",
     "iopub.status.idle": "2023-12-28T17:51:33.445630Z",
     "shell.execute_reply.started": "2023-12-28T17:51:33.439306Z",
     "shell.execute_reply": "2023-12-28T17:51:33.444030Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets\n",
    "!pip install -U accelerate\n",
    "!pip install -U transformers\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "# Allocate maximum CUDA memory reserve in an attempt to prevent CUDA out of memory errors\n",
    "# Reserve is simply the reserved memory, not the in-use memory.\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:24:59.482367900Z",
     "start_time": "2023-12-27T23:24:57.520190900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-08T18:50:25.372470Z",
     "iopub.execute_input": "2024-01-08T18:50:25.372776Z",
     "iopub.status.idle": "2024-01-08T18:51:12.282418Z",
     "shell.execute_reply.started": "2024-01-08T18:50:25.372753Z",
     "shell.execute_reply": "2024-01-08T18:51:12.280506Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0+cpu)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.19.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\nCollecting transformers\n  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/20/0a/739426a81f7635b422fbe6cb8d1d99d1235579a6ac8024c13d743efa6847/transformers-4.36.2-py3-none-any.whl.metadata\n  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m126.8/126.8 kB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.2/8.2 MB\u001B[0m \u001B[31m67.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m0:01\u001B[0m\n\u001B[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.36.0\n    Uninstalling transformers-4.36.0:\n      Successfully uninstalled transformers-4.36.0\nSuccessfully installed transformers-4.36.2\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, the dataset needs to be converted from JSONL to a \"dataset\" object from the `datasets` library.\n",
    "\n",
    "This library provides a `train_test_split` to split the dataset into a test set and training set.\n",
    "\n",
    "I shuffle the dataset first with a fixed seed, so the results are always repoducible."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Dataset files are stored in different locations depending on where the Notebook is ran\n",
    "# Uncomment depending on location:\n",
    "\n",
    "# Kaggle:\n",
    "dataset_location = \"/kaggle/input/terms-privacy-joint/Merged_Dataset.jsonl\"\n",
    "\n",
    "# Google Colab / Running Locally:\n",
    "# dataset_location = \"dataset.json\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=dataset_location,split='train')\n",
    "dataset = dataset.shuffle(seed=2424)\n",
    "dataset = dataset.train_test_split(test_size=0.1, shuffle=False) # disabling shuffling to shuffle with a fixed seed on previous line instead\n",
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:00.536602700Z",
     "start_time": "2023-12-27T23:24:59.485367Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-08T18:51:12.285154Z",
     "iopub.execute_input": "2024-01-08T18:51:12.286246Z",
     "iopub.status.idle": "2024-01-08T18:51:13.724870Z",
     "shell.execute_reply.started": "2024-01-08T18:51:12.286199Z",
     "shell.execute_reply": "2024-01-08T18:51:13.723863Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-db018651d418a8ee/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa8c6df8e2aa45168a7abeb3cd164ebe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec1610be049944a99ca5ead751454acb"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-db018651d418a8ee/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\nDatasetDict({\n    train: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 476\n    })\n    test: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 53\n    })\n})\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset has been split into two sets:\n",
    " - Train\n",
    " - Test\n",
    "\n",
    "The train set will be used to train the model - this is the information the model will learn from.\n",
    "The test set will be used to test the model after training, to see how it performs for some data **It has never seen**\n",
    "\n",
    "The dataset has two features - the \"document\" which is a terms document and then \"summary\" is the respective summary of thwese terms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, a model needs to be selected to conduct transfer learning on.\n",
    "\n",
    "There is a problem here, in that, the collected terms and conditions are **very long**.\n",
    "\n",
    "Below, the length of the first item in the dataset is 81,271 characters whilst the largest is 771,229 characters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "length_of_first_item = len(dataset['train'][0]['document'])\n",
    "print(f'The length of the first document in the train dataset is {length_of_first_item} characters')\n",
    "\n",
    "length_of_longest_document = len(max(dataset['train'], key=lambda x: len(x['document']))['document'])\n",
    "print(f'Length of the longest documentt in the train dataset is {length_of_longest_document} characters')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:00.583600500Z",
     "start_time": "2023-12-27T23:25:00.539603100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-08T18:51:13.727047Z",
     "iopub.execute_input": "2024-01-08T18:51:13.727604Z",
     "iopub.status.idle": "2024-01-08T18:51:13.827143Z",
     "shell.execute_reply.started": "2024-01-08T18:51:13.727562Z",
     "shell.execute_reply": "2024-01-08T18:51:13.825493Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "The length of the first document in the train dataset is 150124 characters\nLength of the longest documentt in the train dataset is 771229 characters\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is problematic as NLP models have a **maximum token count** that it can handle, often much lower than the length of the collected documents, exacerbated by the fact summarisation models typically have lower maximum token counts.\n",
    "\n",
    "This may end up affecting the accuracy of the model and it's ability to learn from the data, as if a portion of the document is cut off to stay within the maximum token count, the summary may not fully match the document.\n",
    "\n",
    "Unfortunately, this is a limitation of NLP as a whole.\n",
    "\n",
    "The maximum length of most of the most common summarization models is 1024 tokens, it's impossible to tell how many characters this is, but a rough heuristic is [1 token = 4 characters](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) \n",
    "\n",
    "The model below thus supports around 4096 characters, which is clearly not good enough for the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "bart_model_checkpoint_name = \"facebook/bart-large-cnn\"\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained(bart_model_checkpoint_name)\n",
    "bart_max_length = bart_tokenizer.model_max_length\n",
    "bart_max_length, bart_max_length*4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:02.663153500Z",
     "start_time": "2023-12-27T23:25:00.587603200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-08T18:51:13.830237Z",
     "iopub.execute_input": "2024-01-08T18:51:13.830649Z",
     "iopub.status.idle": "2024-01-08T18:51:19.822685Z",
     "shell.execute_reply.started": "2024-01-08T18:51:13.830615Z",
     "shell.execute_reply": "2024-01-08T18:51:19.820478Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "803f362501e14c38841c030b57e1227c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c1450096f364222a14b9776a802f0a1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd83bd13fec146a287daa1b9af34754f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15ba35b0cc55407a9525e5f02a5c42c1"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 4,
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1024, 4096)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thus, it's required to look at a bigger model.\n",
    "\n",
    "Below, \"led-large-book-summary\" has a model max length of 16384 tokens, applying the same heuristic as before, this is about 65536 words.\n",
    "\n",
    "This model was trained on the [BookSum dataset](https://arxiv.org/abs/2105.08209) which contains \"plays, short stories, and novels\" with expired copyright, and the aim of the training was to produce valuable summaries of the given documents.\n",
    "\n",
    "I will conduct transfer learning to remove the head of this model which is specifically focused on books, and train on the new task of summarising **privacy policies**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "model_checkpoint_name = \"pszemraj/led-large-book-summary\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_checkpoint_name)\n",
    "tokeniser.model_max_length, tokeniser.model_max_length * 4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:02.850129500Z",
     "start_time": "2023-12-27T23:25:02.666127200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-08T18:51:19.824581Z",
     "iopub.execute_input": "2024-01-08T18:51:19.825246Z",
     "iopub.status.idle": "2024-01-08T18:51:20.639690Z",
     "shell.execute_reply.started": "2024-01-08T18:51:19.825212Z",
     "shell.execute_reply": "2024-01-08T18:51:20.637700Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9075daacb5264cab9667c830a0f06ee2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46e2abd4bef741ffb9bad805c0126297"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58486181fb974cb7a534868e692ddabc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc9f875773574d97b052cd306e804658"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc25e384819348a88ce7eadd78390683"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "(16384, 65536)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above has taken the \"tokeniser\" from the model.\n",
    "\n",
    "Transformer models only take numerical inputs, thus, it's essential to get a numerical representation of the input.\n",
    "\n",
    "The tokeniser is responsible for turning sentences into a series of numbers, called \"tokens\".\n",
    "\n",
    "Tokenisers also use special characters, often to indicate the start and end of sentences or words.\n",
    "\n",
    "All this information helps a NLP neural network form an understanding of the input.\n",
    "\n",
    "The below output shows the tokens used to encode the given test string, with `<s>` being used to indicate the start of a sequence, and each subsequent token beginning with `Ġ`, indicating the start of a new token.\n",
    "\n",
    "Note that `tokeniser` was split into two tokens `token` and `iser`, this is important for tokenisers to be able to re-use tokens wherever possible - `iser` can be used as the suffix for many different words.\n",
    "\n",
    "An attention mask can tell the tokenizer not to pay attention to certain tokens."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "test_string = tokeniser(\"This is a test string to test out the tokeniser\")\n",
    "print(test_string)\n",
    "tokeniser.convert_ids_to_tokens(test_string.input_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:02.865783Z",
     "start_time": "2023-12-27T23:25:02.853128700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-08T18:51:20.641343Z",
     "iopub.execute_input": "2024-01-08T18:51:20.641732Z",
     "iopub.status.idle": "2024-01-08T18:51:20.658915Z",
     "shell.execute_reply.started": "2024-01-08T18:51:20.641701Z",
     "shell.execute_reply": "2024-01-08T18:51:20.657458Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'input_ids': [0, 713, 16, 10, 1296, 6755, 7, 1296, 66, 5, 19233, 5999, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
     "output_type": "stream"
    },
    {
     "execution_count": 6,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['<s>',\n 'This',\n 'Ġis',\n 'Ġa',\n 'Ġtest',\n 'Ġstring',\n 'Ġto',\n 'Ġtest',\n 'Ġout',\n 'Ġthe',\n 'Ġtoken',\n 'iser',\n '</s>']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, recall that the maximum token size for the model was 16384, or about 65536 words.\n",
    "\n",
    "The majority of the scraped documents still exceed this amount, unfortunately, since no bigger summarisation model exists, the input text will need to be truncated down to the maximum token size, which **will have an impact on the model's ability to learn** but there is no other option.\n",
    "\n",
    "Furthermore, the tokenizer will output may different outputs, one of which being `input_ids`, the numerical IDs of the tokens in the tokenised text.\n",
    "\n",
    "The `input_ids` will need to be assigned to the `labels` key of the tokenised input document. This prepares the tokenised input for a task where the model needs to predict the summary, which is how the model will learn.\n",
    "\n",
    "Below defines and runs a function which will truncate the input document, **but not the output** and assign the labels and input id's as necessary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def truncate_input_tokens(input):\n",
    "    truncated_input = tokeniser(\n",
    "        input[\"document\"],\n",
    "        max_length = 16384,\n",
    "        truncation = True\n",
    "    )\n",
    "    labels = tokeniser( # don't truncate output\n",
    "        input[\"summary\"],\n",
    "        truncation = False,\n",
    "    )\n",
    "    truncated_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return truncated_input\n",
    "\n",
    "# By passing `batched = true` into the map function, more than one item is applied to the function at a time.\n",
    "tokenised_dataset = dataset.map(truncate_input_tokens, batched = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:07.763873200Z",
     "start_time": "2023-12-27T23:25:02.868783400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-08T18:51:20.660119Z",
     "iopub.execute_input": "2024-01-08T18:51:20.660425Z",
     "iopub.status.idle": "2024-01-08T18:51:41.496939Z",
     "shell.execute_reply.started": "2024-01-08T18:51:20.660375Z",
     "shell.execute_reply": "2024-01-08T18:51:41.495484Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87e55f5e58904f5f9c8a12ab691008a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a47992fc60684660a7fdae2fe92ee8c7"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below now looks at the new dataset.\n",
    "\n",
    "We can see new features `input_ids`, `attention_mask` and `labels`\n",
    "\n",
    "The lengths of these inputs are also visible, both at 16384 (the maximum) tokens for the first input, and the biggest.\n",
    "\n",
    "The smallest input however, is just 694 tokens, suggesting some documents stayed well under the maximum."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(tokenised_dataset)\n",
    "\n",
    "length_of_first_item = len(tokenised_dataset['train'][0]['input_ids'])\n",
    "print(f'The length of the first document in the train dataset is {length_of_first_item} tokens')\n",
    "\n",
    "length_of_longest_document = len(max(tokenised_dataset['train'], key=lambda x: len(x['input_ids']))['input_ids'])\n",
    "print(f'Length of the longest document in the train dataset is {length_of_longest_document} tokens')\n",
    "\n",
    "length_of_longest_document = len(min(tokenised_dataset['train'], key=lambda x: len(x['input_ids']))['input_ids'])\n",
    "print(f'Length of the smallest document in the train dataset is {length_of_longest_document} tokens')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:12.004420400Z",
     "start_time": "2023-12-27T23:25:07.760854Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-08T18:51:41.498623Z",
     "iopub.execute_input": "2024-01-08T18:51:41.499315Z",
     "iopub.status.idle": "2024-01-08T18:51:51.724310Z",
     "shell.execute_reply.started": "2024-01-08T18:51:41.499269Z",
     "shell.execute_reply": "2024-01-08T18:51:51.723374Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "DatasetDict({\n    train: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 476\n    })\n    test: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 53\n    })\n})\nThe length of the first document in the train dataset is 16384 tokens\nLength of the longest document in the train dataset is 16384 tokens\nLength of the smallest document in the train dataset is 393 tokens\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining Evaluation Techniques"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In Evaluation of summarisation tasks in NLP, the ROUGE metric is most often used.\n",
    "\n",
    "To evaluate the model, I will use the `Hugging Face evaluate` library, as well as the `rouge-score` library to produce the rouge score for the model's summaries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:20.164335900Z",
     "start_time": "2023-12-27T23:25:12.005422Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-08T18:51:51.725672Z",
     "iopub.execute_input": "2024-01-08T18:51:51.727095Z",
     "iopub.status.idle": "2024-01-08T18:52:32.236874Z",
     "shell.execute_reply.started": "2024-01-08T18:51:51.727055Z",
     "shell.execute_reply": "2024-01-08T18:52:32.234773Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Collecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.12.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m2.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.24.3)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=9966a0ffab99024b90e9caef4eeee5dc9af09c67f8963c2637e001a0fc379a34\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2019ba3c6094304bc698d9c32a50b9a"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To calculate the ROUGE metric on predictions, we need the **decoded** predictions and the **decoded** labels, as they are currently *tokenised* (labels simply refers to the expected values).\n",
    "\n",
    "Thus, we need to decode the input tokens back into their English representations.\n",
    "\n",
    "The `batch-decode` function does this, with the parameter `skip_special_tokens` specifying to not handle any special tokens such as `<s>` indicating the start of a sequence. \n",
    "\n",
    "The function below will decode and find the ROUGE score."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def decode_and_find_rouge(result):\n",
    "    predictions,labels = result # extract predictions and labels from the passed in result.\n",
    "    \n",
    "    # Decode by converting tokenised inputs back into English representations\n",
    "    decoded_predictions = tokeniser.batch_decode(predictions, skip_special_tokens=True) \n",
    "    decoded_labels = tokeniser.batch_decode(labels, skip_special_tokens = True)\n",
    "\n",
    "    rouge_score = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "\n",
    "    num_of_predictions = [np.count_nonzero(pred != tokeniser.pad_token_id) for pred in predictions]\n",
    "    rouge_score[\"gen_len\"] = np.mean(num_of_predictions)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in rouge_score.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:20.179299900Z",
     "start_time": "2023-12-27T23:25:20.167300500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-08T18:52:47.140927Z",
     "iopub.execute_input": "2024-01-08T18:52:47.141704Z",
     "iopub.status.idle": "2024-01-08T18:52:47.149783Z",
     "shell.execute_reply.started": "2024-01-08T18:52:47.141668Z",
     "shell.execute_reply": "2024-01-08T18:52:47.147799Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, the model hyper-parameters need to be defined.\n",
    "\n",
    "This is a very expensive model on memory and it is essential to avoid exceeding the GPU's VRAM, thus the hyper-parameters are configured as such:\n",
    "\n",
    "1) The batch size will be set to just 1 to reduce the amount of data stored in memory.\n",
    "\n",
    "2) 16-bit floating point precision will be used, rather than 32-bit.\n",
    "\n",
    "3) The optimiser \"adafactor\" is used over the industry standard of \"Adam\". As a result, the model converges slower but also uses less memory.\n",
    "\n",
    "4) \"Gradient Checkpointing\" is used, instructing the model to forget the majority of forward-pass activations and instead recompute them on demand during the backward pass, saving only the \"most important\" activations. Significantly slower training time, but also saves a lot of VRAM.\n",
    "\n",
    "5) \"Gradient Accumulation\" is used simulate a \"larger\" effective batch size. Instead of updating the model's parameters after processing eachbatch, the gradients are accumulated over 2 batches before performing a single update."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint_name)\n",
    "\n",
    "# Creating a data collator, which will form the batches which will be fed to the model.\n",
    "# It will also conduct padding if necessary to get all inputs of equal length.\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokeniser, model=model_checkpoint_name)\n",
    "\n",
    "models_arguments = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"TOS_8_Epoch_Model\",\n",
    "    evaluation_strategy=\"epoch\", # Run evaluation function on each epoch\n",
    "    learning_rate=2e-5, # learning rate hyperparameter set to 0.00002\n",
    "    per_device_train_batch_size= 1, # split into batches of 1 for training\n",
    "    per_device_eval_batch_size=1, # split to batches of 1 for evaluation\n",
    "    weight_decay=0.01, # Utilises L2 regularization in an attempt to prevent overfitting\n",
    "    save_total_limit=3, # save 3 checkpoints only and delete older checkpoints (Kept using all RAM without)\n",
    "    num_train_epochs=4, # train for 3 epochs\n",
    "    predict_with_generate=True, # Generate summaries for each input ; essential for summarisation tasks\n",
    "    fp16=True, # use 16 bit floating point - reduced memory usage\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\"\n",
    ")\n",
    "\n",
    "# Collect the previously defined trainer parameters, such as the evaluation technique, tokeniser and datasets.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=models_arguments,\n",
    "    train_dataset=tokenised_dataset[\"train\"],\n",
    "    eval_dataset=tokenised_dataset[\"test\"],\n",
    "    tokenizer=tokeniser,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=decode_and_find_rouge,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T23:25:24.378548100Z",
     "start_time": "2023-12-27T23:25:20.183300400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-07T21:46:56.403685Z",
     "iopub.execute_input": "2024-01-07T21:46:56.404043Z",
     "iopub.status.idle": "2024-01-07T21:48:32.769191Z",
     "shell.execute_reply.started": "2024-01-07T21:46:56.404006Z",
     "shell.execute_reply": "2024-01-07T21:48:32.768433Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b114e891233744eebebf740af071535e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9b2021cf43d42a4a8162df87dae10d2"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, the model can be ran using the `train()` function on the trainer.\n",
    "\n",
    "After training, the model will be saved in a file called \"trained_model\".\n",
    " - The size of the model is ~1.7GB"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Uncomment below line if using Kaggle.\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"Joint-4-Epoch_Model\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T23:26:16.985454Z",
     "start_time": "2023-12-27T23:25:42.912321500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-01-07T21:48:32.770380Z",
     "iopub.execute_input": "2024-01-07T21:48:32.770666Z",
     "iopub.status.idle": "2024-01-08T02:37:53.341972Z",
     "shell.execute_reply.started": "2024-01-07T21:48:32.770640Z",
     "shell.execute_reply": "2024-01-08T02:37:53.340667Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a LEDTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='952' max='952' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [952/952 4:48:31, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.647999</td>\n      <td>0.434700</td>\n      <td>0.245400</td>\n      <td>0.249600</td>\n      <td>0.249200</td>\n      <td>135.584900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.530699</td>\n      <td>0.489600</td>\n      <td>0.289900</td>\n      <td>0.259600</td>\n      <td>0.259200</td>\n      <td>226.886800</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.810600</td>\n      <td>0.512794</td>\n      <td>0.504800</td>\n      <td>0.308500</td>\n      <td>0.278500</td>\n      <td>0.278400</td>\n      <td>172.905700</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.810600</td>\n      <td>0.508333</td>\n      <td>0.488900</td>\n      <td>0.296500</td>\n      <td>0.271800</td>\n      <td>0.272300</td>\n      <td>159.434000</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating the model\n",
    "\n",
    "First, we can observe the output of the model on some examples, to see how the model is handling terms and conditions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel,LEDTokenizer, LEDForConditionalGeneration\n",
    "\n",
    "trained_tokeniser = LEDTokenizer.from_pretrained(\"/kaggle/input/joint-4epoch/joint_4_epoch_model\")\n",
    "\n",
    "trained_model = LEDForConditionalGeneration.from_pretrained(\"/kaggle/input/joint-4epoch/joint_4_epoch_model\")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=trained_model,tokenizer=trained_tokeniser)\n",
    "\n",
    "example = dataset[\"test\"][\"document\"][0]\n",
    "\n",
    "tokenised_example = trained_tokeniser(example,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "outputs = trained_model.generate(tokenised_example)\n",
    "\n",
    "\n",
    "print(f'length of example: {len(example)}')\n",
    "\n",
    "tokenised_example_no_tensor = trained_tokeniser(example,truncation=True).input_ids\n",
    "decoded_example = tokeniser.decode(tokenised_example_no_tensor, skip_special_tokens=True)\n",
    "print(f'Length of tokenised example: {len(decoded_example)}')\n",
    "\n",
    "result = trained_tokeniser.decode(outputs[0], skip_special_tokens=True)\n",
    "result"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-08T18:52:55.827946Z",
     "iopub.execute_input": "2024-01-08T18:52:55.828302Z",
     "iopub.status.idle": "2024-01-08T18:55:00.425493Z",
     "shell.execute_reply.started": "2024-01-08T18:52:55.828277Z",
     "shell.execute_reply": "2024-01-08T18:55:00.424166Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "length of example: 23176\nLength of tokenised example: 23176\n",
     "output_type": "stream"
    },
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'The service can sell or otherwise transfer your personal data as part of a bankruptcy proceeding or other type of financial transaction.. The service may use tracking pixels, web beacons, browser fingerprinting, and/or device fingerprinting on users.. This service reserves the right to disclose your personal information without notifying you. Users should revisit the terms periodically, although in case of material changes, the service will notify. Your IP address is collected, which can be used to view your approximate location. There is a date of the last update of the agreements. You must provide your identifiable information'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "second_example = dataset[\"test\"][\"document\"][1]\n",
    "\n",
    "tokenised_example_2 = trained_tokeniser(second_example,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "outputs_2 = trained_model.generate(tokenised_example_2)\n",
    "\n",
    "result = trained_tokeniser.decode(outputs_2[0], skip_special_tokens=True)\n",
    "\n",
    "print(f'Ground truth Summary: {dataset[\"test\"][\"summary\"][1]}')\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(f'produced summary: {result}')\n",
    "\n",
    "print(\" \")\n",
    "      \n",
    "print(f'length of example: {len(second_example)}')\n",
    "tokenised_second_example_no_tensor = trained_tokeniser(second_example,truncation=True).input_ids\n",
    "second_decoded_example = tokeniser.decode(tokenised_second_example_no_tensor, skip_special_tokens=True)\n",
    "print(f'Length of tokenised example: {len(second_decoded_example)}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-08T18:55:00.428243Z",
     "iopub.execute_input": "2024-01-08T18:55:00.428803Z",
     "iopub.status.idle": "2024-01-08T18:55:53.254492Z",
     "shell.execute_reply.started": "2024-01-08T18:55:00.428765Z",
     "shell.execute_reply": "2024-01-08T18:55:53.253572Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": "Ground truth Summary: The service explains how to prevent disclosure of personal information to third parties. This service keeps user logs for an undefined period of time. Blocking cookies may limit your ability to use the service.  Terms may be changed any time at their discretion, without notice to the user . The service uses your personal data to employ targeted third-party advertising\n\nproduced summary: Your IP address is collected, which can be used to view your approximate location. Logs are kept for an undefined period of time. Third-party cookies are used for statistics. This service tracks which web page referred you to it. You can opt out of targeted advertising. Your personal data is aggregated into statistics. Information is gathered about you through third parties. Blocking first party cookies may limit your ability to use the service. Details are provided about what kind of information they collect. Tracking pixels are used in service-to-user communication. Do Not Track (DNT) headers are ignored and you are tracked anyway even if you set this header..  Terms may be changed any time at their discretion, without notice to you. There is a date of the last update of the agreements. Many different types of personal data are collected\n \nlength of example: 9499\nLength of tokenised example: 9499\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "lorem_example = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\"\n",
    "\n",
    "tokenised_example_lorem = trained_tokeniser(lorem_example,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "outputs_lorem = trained_model.generate(tokenised_example_lorem)\n",
    "trained_tokeniser.decode(outputs_lorem[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-08T18:55:53.256715Z",
     "iopub.execute_input": "2024-01-08T18:55:53.257521Z",
     "iopub.status.idle": "2024-01-08T18:56:18.602370Z",
     "shell.execute_reply.started": "2024-01-08T18:55:53.257476Z",
     "shell.execute_reply": "2024-01-08T18:56:18.601460Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "execution_count": 13,
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"The service is provided 'as is' and to be used at the users' sole risk. You are responsible for maintaining the security of your account and for the activities on your account. The court of law governing the terms is in a jurisdiction that is friendlier to user privacy protection (California, USA). Instead of asking directly, this Service will assume your consent merely from your usage.. This service does not condone any ideas contained in its user-generated contents.\""
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the surface these summaries look sufficient.\n",
    "\n",
    "We can look at the ROUGE scores of a test set example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "example_document = dataset[\"test\"][\"document\"][5]\n",
    "ground_truth_summary = dataset[\"test\"][\"summary\"][5]\n",
    "\n",
    "tokenised_document = trained_tokeniser(example_document,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "tokenised_ground_truth = trained_tokeniser(ground_truth_summary,return_tensors=\"pt\",truncation=True).input_ids\n",
    "\n",
    "output_summary = trained_model.generate(tokenised_document)\n",
    "\n",
    "decoded_summary = trained_tokeniser.decode(output_summary[0], skip_special_tokens=True)\n",
    "\n",
    "# Rouge metric expects array.\n",
    "truth = [ground_truth_summary]\n",
    "summary = [decoded_summary]\n",
    "\n",
    "rouge_score = rouge.compute(predictions=truth, references=summary)\n",
    "\n",
    "print(rouge_score)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-08T18:56:18.604515Z",
     "iopub.execute_input": "2024-01-08T18:56:18.605083Z",
     "iopub.status.idle": "2024-01-08T18:57:35.591120Z",
     "shell.execute_reply.started": "2024-01-08T18:56:18.605050Z",
     "shell.execute_reply": "2024-01-08T18:57:35.589447Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'rouge1': 0.5586206896551724, 'rouge2': 0.3055555555555555, 'rougeL': 0.2620689655172414, 'rougeLsum': 0.2620689655172414}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now take the ROUGE score across the entire test set\n",
    "\n",
    "This will take a while, due to needing to create summaries for the entire test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "summaries = []\n",
    "truths = []\n",
    "count = 1\n",
    "\n",
    "for item in dataset[\"test\"]:\n",
    "    print(f'Processing document {count} of {len(dataset[\"test\"])}')\n",
    "    document = item[\"document\"]\n",
    "    ground_truth_summary = item[\"summary\"]\n",
    "    \n",
    "    tokenised_document = trained_tokeniser(document,return_tensors=\"pt\",truncation=True).input_ids\n",
    "    generated_summary = trained_model.generate(tokenised_document)\n",
    "    decoded_summary = trained_tokeniser.decode(generated_summary[0], skip_special_tokens=True)\n",
    "    \n",
    "    summaries.append(decoded_summary)\n",
    "    truths.append(ground_truth_summary)\n",
    "    count += 1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-08T18:57:35.593326Z",
     "iopub.execute_input": "2024-01-08T18:57:35.593777Z",
     "iopub.status.idle": "2024-01-08T21:28:40.052468Z",
     "shell.execute_reply.started": "2024-01-08T18:57:35.593745Z",
     "shell.execute_reply": "2024-01-08T21:28:40.050966Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": "Processing document 1 of 53\nProcessing document 2 of 53\nProcessing document 3 of 53\nProcessing document 4 of 53\nProcessing document 5 of 53\nProcessing document 6 of 53\nProcessing document 7 of 53\nProcessing document 8 of 53\nProcessing document 9 of 53\nProcessing document 10 of 53\nProcessing document 11 of 53\nProcessing document 12 of 53\nProcessing document 13 of 53\nProcessing document 14 of 53\nProcessing document 15 of 53\nProcessing document 16 of 53\nProcessing document 17 of 53\nProcessing document 18 of 53\nProcessing document 19 of 53\nProcessing document 20 of 53\nProcessing document 21 of 53\nProcessing document 22 of 53\nProcessing document 23 of 53\nProcessing document 24 of 53\nProcessing document 25 of 53\nProcessing document 26 of 53\nProcessing document 27 of 53\nProcessing document 28 of 53\nProcessing document 29 of 53\nProcessing document 30 of 53\nProcessing document 31 of 53\nProcessing document 32 of 53\nProcessing document 33 of 53\nProcessing document 34 of 53\nProcessing document 35 of 53\nProcessing document 36 of 53\nProcessing document 37 of 53\nProcessing document 38 of 53\nProcessing document 39 of 53\nProcessing document 40 of 53\nProcessing document 41 of 53\nProcessing document 42 of 53\nProcessing document 43 of 53\nProcessing document 44 of 53\nProcessing document 45 of 53\nProcessing document 46 of 53\nProcessing document 47 of 53\nProcessing document 48 of 53\nProcessing document 49 of 53\nProcessing document 50 of 53\nProcessing document 51 of 53\nProcessing document 52 of 53\nProcessing document 53 of 53\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "rouge_score = rouge.compute(predictions=summaries,references=truths)\n",
    "print(rouge_score)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-08T21:28:40.053904Z",
     "iopub.execute_input": "2024-01-08T21:28:40.054317Z",
     "iopub.status.idle": "2024-01-08T21:28:41.429470Z",
     "shell.execute_reply.started": "2024-01-08T21:28:40.054289Z",
     "shell.execute_reply": "2024-01-08T21:28:41.428293Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'rouge1': 0.48945577877779906, 'rouge2': 0.2953093698427508, 'rougeL': 0.27031254897617607, 'rougeLsum': 0.27094568586828716}\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
