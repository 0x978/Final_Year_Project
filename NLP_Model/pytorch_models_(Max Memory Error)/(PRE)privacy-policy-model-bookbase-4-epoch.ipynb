{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7293442,
     "sourceType": "datasetVersion",
     "datasetId": 4230174
    },
    {
     "sourceId": 7293534,
     "sourceType": "datasetVersion",
     "datasetId": 4230224
    },
    {
     "sourceId": 7300111,
     "sourceType": "datasetVersion",
     "datasetId": 4234911
    },
    {
     "sourceId": 7392147,
     "sourceType": "datasetVersion",
     "datasetId": 4297367
    }
   ],
   "dockerImageVersionId": 30627,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install required libraries and imports."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets\n",
    "!pip install -U transformers\n",
    "!pip3 install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -U datasets\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "# Allocate maximum CUDA memory reserve in an attempt to prevent CUDA out of memory errors\n",
    "# Reserve is simply the reserved memory, not the in-use memory.os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:21.193654300Z",
     "start_time": "2024-01-13T01:36:14.536744900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:33:45.904874Z",
     "iopub.execute_input": "2024-01-13T03:33:45.905275Z",
     "iopub.status.idle": "2024-01-13T03:34:36.369330Z",
     "shell.execute_reply.started": "2024-01-13T03:33:45.905243Z",
     "shell.execute_reply": "2024-01-13T03:34:36.368367Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nLooking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting datasets\n  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/ec/93/454ada0d1b289a0f4a86ac88dbdeab54921becabac45da3da787d136628f/datasets-2.16.1-py3-none-any.whl.metadata\n  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.12.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nCollecting pyarrow-hotfix (from datasets)\n  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nCollecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets)\n  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m507.1/507.1 kB\u001B[0m \u001B[31m35.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.4/166.4 kB\u001B[0m \u001B[31m20.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: pyarrow-hotfix, fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.12.2\n    Uninstalling fsspec-2023.12.2:\n      Successfully uninstalled fsspec-2023.12.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed datasets-2.16.1 fsspec-2023.10.0 pyarrow-hotfix-0.6\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tell PyTorch to use GPU wherever possible"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:14:28.292515800Z",
     "start_time": "2024-01-13T02:14:28.282516500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:34:36.371211Z",
     "iopub.execute_input": "2024-01-13T03:34:36.372227Z",
     "iopub.status.idle": "2024-01-13T03:34:39.669104Z",
     "shell.execute_reply.started": "2024-01-13T03:34:36.372191Z",
     "shell.execute_reply": "2024-01-13T03:34:39.668305Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, the dataset and checkpoint needs to be initialised."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset location\n",
    "Dataset location depends on where notebook is running, for ease I set it up to just uncomment line below depending on location the notebook is running as I run the notebook in a lot of locations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_location = \"/kaggle/input/new-datasets/Privacy_Policy_dataset.jsonl\" # Kaggle\n",
    "\n",
    "#dataset_location = \"Privacy_Policy_dataset.jsonl\" # Local / Google Colab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:21.238664900Z",
     "start_time": "2024-01-13T01:36:21.195654200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:34:39.670233Z",
     "iopub.execute_input": "2024-01-13T03:34:39.670843Z",
     "iopub.status.idle": "2024-01-13T03:34:39.675281Z",
     "shell.execute_reply.started": "2024-01-13T03:34:39.670804Z",
     "shell.execute_reply": "2024-01-13T03:34:39.674269Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialise dataset\n",
    "To initialise the dataset I use the \"dataset\" library from python.\n",
    "\n",
    "I split the dataset into three sets:\n",
    "- Training set - The data shown to the model during training\n",
    "- Validation - The data shown to the model to calculate loss on backward pass\n",
    "- Test - Reserved strictly for after the model is trained, used to evaluate the model on a completely unseen set\n",
    "\n",
    "However, the \"datasets\" library doesn't offer the possibility to split into three sets so I use a workaround sourced from: [This stackoverflow post](https://stackoverflow.com/questions/76001128/splitting-dataset-into-train-test-and-validation-using-huggingface-datasets-fun)\n",
    "\n",
    "It works by first splitting the data set into a train set (80%) and a validation set (20%).\n",
    "\n",
    "It then splits this validation set into a train set and validation set of 50% each, resulting in two sets of 10% each.\n",
    "\n",
    "A final dataset is then built using these split datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"json\", data_files=dataset_location,split='train')\n",
    "\n",
    "dataset = dataset.shuffle(seed=2424)\n",
    "\n",
    "test_valid_split_dataset = dataset.train_test_split(test_size=0.2, shuffle=False)\n",
    "\n",
    "test_split = test_valid_split_dataset['test'].train_test_split(test_size=0.5, shuffle = False)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': test_valid_split_dataset['train'],\n",
    "    'test': test_split['test'],\n",
    "    'valid': test_split['train']})\n",
    "\n",
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:21.948222400Z",
     "start_time": "2024-01-13T01:36:21.212681500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:34:39.677331Z",
     "iopub.execute_input": "2024-01-13T03:34:39.677619Z",
     "iopub.status.idle": "2024-01-13T03:34:40.825614Z",
     "shell.execute_reply.started": "2024-01-13T03:34:39.677595Z",
     "shell.execute_reply": "2024-01-13T03:34:40.824764Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9937afea4d4d4dafb1ec4bae9d5c4bdf"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "DatasetDict({\n    train: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 453\n    })\n    test: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 57\n    })\n    valid: Dataset({\n        features: ['summary', 'document'],\n        num_rows: 57\n    })\n})\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Review dataset\n",
    "Next I want to see the properties of the dataset, to understand what i'm working with.\n",
    "\n",
    "For training a summarisation model knowing the length of the collected documents is crucial.\n",
    "\n",
    "The largest summarisation base model is only capable of processing 16384 tokens - higher token limits is a limitation in NLP as a whole.\n",
    "\n",
    "Roughly we can equate [One token to be equal to about 4 English characters)[https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them].\n",
    "\n",
    "This gives roughly 65,536 characters which the model will be able to parse at once, and anything which exceeds this number needs to be truncated down to the 16384 token limit.\n",
    "\n",
    "This unfortunately means on some documents, some detail will be missing.\n",
    "\n",
    "However, as visible below, the average document is just 24,724 considerably under the maximum token limit, thus, for most items in the dataset this isn't a problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "length_of_first_item = len(dataset['train'][0]['document'])\n",
    "print(f'The length of the first privacy policy in the train dataset is {length_of_first_item} characters')\n",
    "\n",
    "length_of_longest_document = len(max(dataset['train'], key=lambda x: len(x['document']))['document'])\n",
    "print(f'Length of the longest privacy policy in the train dataset is {length_of_longest_document} characters')\n",
    "\n",
    "length_of_shortest_document = len(min(dataset['train'], key=lambda x: len(x['document']))['document'])\n",
    "print(f'Length of the shortest privacy policy in the train dataset is {length_of_shortest_document} characters')\n",
    "\n",
    "total_char_count = sum(map(len, dataset['train']['document']))\n",
    "avg_char_count = round(total_char_count / len(dataset['train']['document']))\n",
    "\n",
    "print(f'The average character count in the dataset is: {avg_char_count}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.059222300Z",
     "start_time": "2024-01-13T01:36:21.951221600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:34:40.826769Z",
     "iopub.execute_input": "2024-01-13T03:34:40.827104Z",
     "iopub.status.idle": "2024-01-13T03:34:40.966313Z",
     "shell.execute_reply.started": "2024-01-13T03:34:40.827078Z",
     "shell.execute_reply": "2024-01-13T03:34:40.965440Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "The length of the first privacy policy in the train dataset is 5342 characters\nLength of the longest privacy policy in the train dataset is 261619 characters\nLength of the shortest privacy policy in the train dataset is 826 characters\nThe average character count in the dataset is: 24724\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Base model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I will be utilising transfer learning to train a model.\n",
    "\n",
    "This takes the base of a model trained on some other task but in a similar domain (e.g. summarising books), removes the head of the model which is more specialised (e.g. contains information specific to books), while retaining useful information about the English language. The model is then trained on a new specific task, in my case, summarising terms and conditions or privacy policies, utilising its pre-existing knowledge of the English language.\n",
    "\n",
    "This significantly reduces training time and resources required for training such that I can stay within the final year project deadlines.\n",
    "\n",
    "The model I will be using as a base is the [led-large-book-summary](https://huggingface.co/pszemraj/led-large-book-summary). This model utilises the Longformer Encoder-Decoder (LED) model as it's base, and was trained further to summarise long-form text such as novels, plays and stories from the [BookSum dataset](https://arxiv.org/abs/2105.08209)\n",
    "\n",
    " Below I initialise the tokeniser for this model through the [Hugging Face](https://huggingface.co/models) library, which offer a variety of base models for transfer-learning.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_name = \"pszemraj/led-large-book-summary\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(base_model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.618471300Z",
     "start_time": "2024-01-13T01:36:22.061221600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:34:40.967369Z",
     "iopub.execute_input": "2024-01-13T03:34:40.967637Z",
     "iopub.status.idle": "2024-01-13T03:34:45.969236Z",
     "shell.execute_reply.started": "2024-01-13T03:34:40.967615Z",
     "shell.execute_reply": "2024-01-13T03:34:45.968422Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0dfc216e5cca45ddb416c159159be256"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0709f4f62fd94204b33cdf7dc970d508"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00c1f2a383a04f0dbcb8365096097727"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3d1ae84d3b748a2896f1de864ec9c6c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad008474b053436b878235570141470f"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformer models *only* take numerical inputs, thus, a tokeniser is responsible for transforming the input text into its numerical representation.\n",
    "\n",
    "It does this by splitting text into \"tokens\" which are small groups of characters.\n",
    "\n",
    "Tokenisers also use special characters, often indicating the start and the end of sequences and words.\n",
    "\n",
    "Below I display an example by tokenising a simple string using the tokeniser for the \"led-large-book-summary\" model\n",
    "\n",
    "The `input_ids` represents the tokenised input, `attention_mask` is tells the model to ignore tokens if the equivalent index in the attention mask array is zero.\n",
    "\n",
    "Note when converting the string back to English, we can see`<s>` being used to indicate the start of a sequence, and each word beginning with `Ġ`.\n",
    " - This changes depending on the base model, but the Hugging Face library picks out the right tokeniser for the base model.\n",
    "\n",
    "Also, words can be split into two tokens if it is deemed useful, below `tokeniser` is split into two tokens `token` and `iser`, as the tokens `iser` and `token` could later be re-used with other words, saving tokens."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "test_string = tokeniser(\"This is a test string to test out the tokeniser\")\n",
    "print(test_string)\n",
    "tokeniser.convert_ids_to_tokens(test_string.input_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.629472300Z",
     "start_time": "2024-01-13T01:36:22.614472800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:34:45.970357Z",
     "iopub.execute_input": "2024-01-13T03:34:45.970830Z",
     "iopub.status.idle": "2024-01-13T03:34:45.981928Z",
     "shell.execute_reply.started": "2024-01-13T03:34:45.970803Z",
     "shell.execute_reply": "2024-01-13T03:34:45.980946Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'input_ids': [0, 713, 16, 10, 1296, 6755, 7, 1296, 66, 5, 19233, 5999, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
     "output_type": "stream"
    },
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['<s>',\n 'This',\n 'Ġis',\n 'Ġa',\n 'Ġtest',\n 'Ġstring',\n 'Ġto',\n 'Ġtest',\n 'Ġout',\n 'Ġthe',\n 'Ġtoken',\n 'iser',\n '</s>']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, a function is needed which will tokenise the text.\n",
    "\n",
    "Here, the maximum length can be defined as 16384 tokens and the tokeniser will be responsible for ensuring any text exceeding this is truncated.\n",
    "\n",
    "In the dataset, the \"document\" column contains the input document (privacy policy or Terms of Service)\n",
    "\n",
    "The \"summary\" column contains the ground truth summary for the matching document.\n",
    " - This doesn't need to be truncated, as they are all <500 tokens\n",
    "\n",
    "Furthermore, this function assigns the tokenised ground truth summaries `input_id`'s to the \"labels\" property of the tokenised documents.\n",
    " - This is the format required for training a summarisaiton model in pytorch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenise_truncate_dataset(input):\n",
    "    truncated_input = tokeniser(\n",
    "        input[\"document\"],\n",
    "        max_length = 16384,\n",
    "        truncation = True\n",
    "    )\n",
    "    labels = tokeniser( # don't truncate labels\n",
    "        input[\"summary\"],\n",
    "        truncation = False,\n",
    "    )\n",
    "    truncated_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return truncated_input\n",
    "# By passing `batched = true` into the map function, more than one item is applied to the function at a time.\n",
    "tokenised_dataset = dataset.map(tokenise_truncate_dataset, batched = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.723472600Z",
     "start_time": "2024-01-13T01:36:22.631472300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:34:45.983024Z",
     "iopub.execute_input": "2024-01-13T03:34:45.983331Z",
     "iopub.status.idle": "2024-01-13T03:34:51.091787Z",
     "shell.execute_reply.started": "2024-01-13T03:34:45.983298Z",
     "shell.execute_reply": "2024-01-13T03:34:51.090815Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/453 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86d9039275ad47c48d3a7c6c51d998a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/57 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3233fe2d86443edb1786360dbea4d1b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/57 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d19d1dc9574740babf085384e0e1bcee"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The properties of the dataset can now be viewed, as expected, there is a train, test and validation dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "tokenised_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.740471900Z",
     "start_time": "2024-01-13T01:36:22.725471800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:34:51.093167Z",
     "iopub.execute_input": "2024-01-13T03:34:51.093777Z",
     "iopub.status.idle": "2024-01-13T03:34:51.100242Z",
     "shell.execute_reply.started": "2024-01-13T03:34:51.093742Z",
     "shell.execute_reply": "2024-01-13T03:34:51.099349Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "execution_count": 10,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 453\n    })\n    test: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 57\n    })\n    valid: Dataset({\n        features: ['summary', 'document', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 57\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, the \"summary\" and \"document\" dataset are no longer needed, as we have their tokenised equivalents - \"input_ids\" and \"labels\".\n",
    "\n",
    "Thus, these can be removed.\n",
    "\n",
    "Furthermore, the dataset needs to be set to return pytorch tensors, in order to be able to be trained in pytorch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "tokenised_dataset = tokenised_dataset.remove_columns([\"summary\",\"document\"])\n",
    "tokenised_dataset.set_format(\"torch\")\n",
    "tokenised_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:36:22.783937100Z",
     "start_time": "2024-01-13T01:36:22.741474200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:34:51.104258Z",
     "iopub.execute_input": "2024-01-13T03:34:51.104635Z",
     "iopub.status.idle": "2024-01-13T03:34:51.121731Z",
     "shell.execute_reply.started": "2024-01-13T03:34:51.104611Z",
     "shell.execute_reply": "2024-01-13T03:34:51.120722Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 453\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 57\n    })\n    valid: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 57\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, a data collator needs to be defined.\n",
    "\n",
    "(Below Information sourced from: https://huggingface.co/docs/transformers/main_classes/data_collator)\n",
    "\n",
    "This is responsible for constructing batches and applying pre-processing such as padding to ensure all inputs are of the same size.\n",
    "\n",
    "The Hugging Face library provides a function for sourcing a data collator with padding."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:42:02.436350100Z",
     "start_time": "2024-01-13T01:42:01.202269900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:34:51.122829Z",
     "iopub.execute_input": "2024-01-13T03:34:51.123379Z",
     "iopub.status.idle": "2024-01-13T03:35:00.988664Z",
     "shell.execute_reply.started": "2024-01-13T03:34:51.123337Z",
     "shell.execute_reply": "2024-01-13T03:35:00.987853Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, Pytorch dataloaders need to be defined, in order to load the data into the model.\n",
    "\n",
    "A *very* small batch size is used, of just 1, as training this model uses *a lot* of GPU RAM, and anything higher than this causes Kaggle / Colab to crash."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenised_dataset[\"train\"], batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenised_dataset[\"valid\"], batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenised_dataset[\"test\"], batch_size=1, collate_fn=data_collator\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T01:42:53.432954300Z",
     "start_time": "2024-01-13T01:42:53.420958Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:35:00.992829Z",
     "iopub.execute_input": "2024-01-13T03:35:00.993382Z",
     "iopub.status.idle": "2024-01-13T03:35:00.999189Z",
     "shell.execute_reply.started": "2024-01-13T03:35:00.993347Z",
     "shell.execute_reply": "2024-01-13T03:35:00.998291Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "First, the base model needs to be defined.\n",
    "\n",
    "Next, an optimiser needs to be defined.\n",
    "\n",
    "The standard optimiser to use is `adamW`, but again, due to VRAM limitations, a less memory-intensive optimiser will be used.\n",
    "\n",
    "The learning rate and base model parameters will be passed to the optimiser.\n",
    "\n",
    "relative_step will be set to false to define a custom learning rate, which I used in previous models and found to be good."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "from transformers.optimization import Adafactor\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "optimiser = Adafactor(base_model.parameters(),lr=2e-5, relative_step=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:01:43.238089600Z",
     "start_time": "2024-01-13T02:01:40.995123800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:35:01.000271Z",
     "iopub.execute_input": "2024-01-13T03:35:01.000583Z",
     "iopub.status.idle": "2024-01-13T03:36:56.516527Z",
     "shell.execute_reply.started": "2024-01-13T03:35:01.000559Z",
     "shell.execute_reply": "2024-01-13T03:36:56.515724Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5069b7aab3d8448f90e78ebb368b7c8d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbf6c3b9667c45029881e6214d953892"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "(This stackoverflow post helped with defining training steps https://stackoverflow.com/questions/60120043/optimizer-and-scheduler-for-bert-fine-tuning)\n",
    "\n",
    "Next, the *learning rate scheduler* needs to be defined, which is responsible for reducing this learning rate as training continues.\n",
    "\n",
    "The simplest implementation is to just handle this linearly by multiplying the number of epochs by the number of training items, to calculate the number of \"training steps\", then define a learning rate scheduler to handle this linearly.\n",
    "\n",
    "The number of warm up steps is typically set to zero."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers.optimization import get_scheduler\n",
    "\n",
    "epochs = 4\n",
    "training_steps = epochs * len(train_dataloader)\n",
    "learning_rate_scheduler = get_scheduler(\"linear\",optimizer=optimiser,num_warmup_steps=0,num_training_steps=training_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:12:33.630546100Z",
     "start_time": "2024-01-13T02:12:33.609547200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:36:56.517705Z",
     "iopub.execute_input": "2024-01-13T03:36:56.518006Z",
     "iopub.status.idle": "2024-01-13T03:36:56.523139Z",
     "shell.execute_reply.started": "2024-01-13T03:36:56.517981Z",
     "shell.execute_reply": "2024-01-13T03:36:56.522092Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, the base model needs to be assigned to run the GPU\n",
    "\n",
    "If the output is 'cuda' then a GPU is assigned."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "base_model.to(device)\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:14:20.537563700Z",
     "start_time": "2024-01-13T02:14:20.522563800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:36:56.524237Z",
     "iopub.execute_input": "2024-01-13T03:36:56.524488Z",
     "iopub.status.idle": "2024-01-13T03:36:57.138036Z",
     "shell.execute_reply.started": "2024-01-13T03:36:56.524467Z",
     "shell.execute_reply": "2024-01-13T03:36:57.137155Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": [
    {
     "execution_count": 16,
     "output_type": "execute_result",
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, ROUGE needs to be defined for the evaluation loop."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T02:18:36.724807300Z",
     "start_time": "2024-01-13T02:18:26.022709800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:36:57.139060Z",
     "iopub.execute_input": "2024-01-13T03:36:57.139322Z",
     "iopub.status.idle": "2024-01-13T03:37:28.431080Z",
     "shell.execute_reply.started": "2024-01-13T03:36:57.139300Z",
     "shell.execute_reply": "2024-01-13T03:37:28.430070Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Collecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.16.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.12.2)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.24.3)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=2a208112f104aef51746e2c7e4c7838199c90de6abb6a1ef28f3ae4af53f5ca1\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fe6846f0fb8407d83323d7ce6b45cd6"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, the model training loop needs to be defined:\n",
    "\n",
    "By default PyTorch doesn't offer any visualisation of model training progress, so the library `tqdm` is used to visualise this"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "progress_bar = tqdm(range(training_steps))\n",
    "\n",
    "base_model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = base_model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "        learning_rate_scheduler.step()\n",
    "        optimiser.zero_grad()\n",
    "        progress_bar.update(1)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:37:28.432444Z",
     "iopub.execute_input": "2024-01-13T03:37:28.432774Z",
     "iopub.status.idle": "2024-01-13T03:37:32.679825Z",
     "shell.execute_reply.started": "2024-01-13T03:37:28.432746Z",
     "shell.execute_reply": "2024-01-13T03:37:32.678282Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1812 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c464efe78e6745f681a5dbb043ff510e"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a LEDTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[1;32m      7\u001B[0m     batch \u001B[38;5;241m=\u001B[39m {k: v\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m----> 8\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mbase_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m     10\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:2404\u001B[0m, in \u001B[0;36mLEDForConditionalGeneration.forward\u001B[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   2399\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m decoder_input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m decoder_inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2400\u001B[0m         decoder_input_ids \u001B[38;5;241m=\u001B[39m shift_tokens_right(\n\u001B[1;32m   2401\u001B[0m             labels, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpad_token_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mdecoder_start_token_id\n\u001B[1;32m   2402\u001B[0m         )\n\u001B[0;32m-> 2404\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mled\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2405\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2406\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2407\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2408\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2409\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2410\u001B[0m \u001B[43m    \u001B[49m\u001B[43mglobal_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mglobal_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2411\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2412\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2413\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2414\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2415\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2416\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2417\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2418\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2419\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2420\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2421\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2422\u001B[0m lm_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(outputs[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinal_logits_bias\n\u001B[1;32m   2424\u001B[0m masked_lm_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:2254\u001B[0m, in \u001B[0;36mLEDModel.forward\u001B[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   2249\u001B[0m     decoder_input_ids \u001B[38;5;241m=\u001B[39m shift_tokens_right(\n\u001B[1;32m   2250\u001B[0m         input_ids, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpad_token_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mdecoder_start_token_id\n\u001B[1;32m   2251\u001B[0m     )\n\u001B[1;32m   2253\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m encoder_outputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 2254\u001B[0m     encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2255\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2256\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2257\u001B[0m \u001B[43m        \u001B[49m\u001B[43mglobal_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mglobal_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2258\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2259\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2260\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2261\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2262\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2263\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2264\u001B[0m \u001B[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a LEDEncoderBaseModelOutput when return_dict=False\u001B[39;00m\n\u001B[1;32m   2265\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m return_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(encoder_outputs, LEDEncoderBaseModelOutput):\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:1872\u001B[0m, in \u001B[0;36mLEDEncoder.forward\u001B[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1861\u001B[0m         layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m   1862\u001B[0m             encoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m   1863\u001B[0m             hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1869\u001B[0m             output_attentions,\n\u001B[1;32m   1870\u001B[0m         )\n\u001B[1;32m   1871\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1872\u001B[0m         layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mencoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1873\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mis_index_masked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_index_masked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1877\u001B[0m \u001B[43m            \u001B[49m\u001B[43mis_index_global_attn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_index_global_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mis_global_attn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_global_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1879\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1880\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1881\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1883\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n\u001B[1;32m   1884\u001B[0m     \u001B[38;5;66;03m# bzs x seq_len x num_attn_heads x (num_global_attn + attention_window_len + 1) => bzs x num_attn_heads x seq_len x (num_global_attn + attention_window_len + 1)\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:958\u001B[0m, in \u001B[0;36mLEDEncoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001B[0m\n\u001B[1;32m    949\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    950\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    951\u001B[0m \u001B[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape *(batch, seq_len, embed_dim)*\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    955\u001B[0m \u001B[38;5;124;03m        *(encoder_attention_heads,)*.\u001B[39;00m\n\u001B[1;32m    956\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    957\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[0;32m--> 958\u001B[0m attn_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    959\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    960\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    961\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    962\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_index_masked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_index_masked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    963\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_index_global_attn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_index_global_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    964\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_global_attn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_global_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    965\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    966\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    967\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    968\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mdropout(hidden_states, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:767\u001B[0m, in \u001B[0;36mLEDEncoderAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001B[0m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    756\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    757\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    763\u001B[0m     output_attentions: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    764\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, Optional[torch\u001B[38;5;241m.\u001B[39mTensor], Optional[Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]]]:\n\u001B[1;32m    765\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Input shape: Batch x Time x Channel\"\"\"\u001B[39;00m\n\u001B[0;32m--> 767\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlongformer_self_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    768\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    769\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    770\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    771\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_index_masked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_index_masked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    772\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_index_global_attn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_index_global_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    773\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_global_attn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_global_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    774\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    775\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    777\u001B[0m     attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m    778\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attn_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:188\u001B[0m, in \u001B[0;36mLEDEncoderSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001B[0m\n\u001B[1;32m    185\u001B[0m query_vectors \u001B[38;5;241m=\u001B[39m query_vectors\u001B[38;5;241m.\u001B[39mview(seq_len, batch_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    186\u001B[0m key_vectors \u001B[38;5;241m=\u001B[39m key_vectors\u001B[38;5;241m.\u001B[39mview(seq_len, batch_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 188\u001B[0m attn_scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sliding_chunks_query_key_matmul\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_vectors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_vectors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mone_sided_attn_window_size\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;66;03m# values to pad for attention probs\u001B[39;00m\n\u001B[1;32m    193\u001B[0m remove_from_windowed_attention_mask \u001B[38;5;241m=\u001B[39m (attention_mask \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m)[:, :, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m]\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:463\u001B[0m, in \u001B[0;36mLEDEncoderSelfAttention._sliding_chunks_query_key_matmul\u001B[0;34m(self, query, key, window_overlap)\u001B[0m\n\u001B[1;32m    460\u001B[0m diagonal_chunked_attention_scores \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbcxd,bcyd->bcxy\u001B[39m\u001B[38;5;124m\"\u001B[39m, (query, key))  \u001B[38;5;66;03m# multiply\u001B[39;00m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;66;03m# convert diagonals into columns\u001B[39;00m\n\u001B[0;32m--> 463\u001B[0m diagonal_chunked_attention_scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_pad_and_transpose_last_two_dims\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    464\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdiagonal_chunked_attention_scores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    465\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;66;03m# allocate space for the overall attention matrix where the chunks are combined. The last dimension\u001B[39;00m\n\u001B[1;32m    468\u001B[0m \u001B[38;5;66;03m# has (window_overlap * 2 + 1) columns. The first (window_overlap) columns are the window_overlap lower triangles (attention from a word to\u001B[39;00m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;66;03m# window_overlap previous words). The following column is attention score from each word to itself, then\u001B[39;00m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;66;03m# followed by window_overlap columns for the upper triangle.\u001B[39;00m\n\u001B[1;32m    472\u001B[0m diagonal_attention_scores \u001B[38;5;241m=\u001B[39m diagonal_chunked_attention_scores\u001B[38;5;241m.\u001B[39mnew_zeros(\n\u001B[1;32m    473\u001B[0m     (batch_size \u001B[38;5;241m*\u001B[39m num_heads, chunks_count \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, window_overlap, window_overlap \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    474\u001B[0m )\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:321\u001B[0m, in \u001B[0;36mLEDEncoderSelfAttention._pad_and_transpose_last_two_dims\u001B[0;34m(hidden_states_padded, padding)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_pad_and_transpose_last_two_dims\u001B[39m(hidden_states_padded, padding):\n\u001B[1;32m    320\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"pads rows and then flips rows and columns\"\"\"\u001B[39;00m\n\u001B[0;32m--> 321\u001B[0m     hidden_states_padded \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    322\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states_padded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# padding value is not important because it will be overwritten\u001B[39;00m\n\u001B[1;32m    324\u001B[0m     hidden_states_padded \u001B[38;5;241m=\u001B[39m hidden_states_padded\u001B[38;5;241m.\u001B[39mview(\n\u001B[1;32m    325\u001B[0m         \u001B[38;5;241m*\u001B[39mhidden_states_padded\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m], hidden_states_padded\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), hidden_states_padded\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    326\u001B[0m     )\n\u001B[1;32m    327\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states_padded\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1.06 GiB (GPU 0; 15.89 GiB total capacity; 14.18 GiB already allocated; 960.12 MiB free; 14.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ],
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.06 GiB (GPU 0; 15.89 GiB total capacity; 14.18 GiB already allocated; 960.12 MiB free; 14.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, the evaluation loop to be used during model training needs to be defined.\n",
    "\n",
    "This will run against the evaluation set (eval_dataloader) defined earlier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "base_model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = base_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    rouge.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "rouge.compute()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-01-13T03:37:32.680813Z",
     "iopub.status.idle": "2024-01-13T03:37:32.681167Z",
     "shell.execute_reply.started": "2024-01-13T03:37:32.681001Z",
     "shell.execute_reply": "2024-01-13T03:37:32.681017Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
